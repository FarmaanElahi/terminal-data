{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz2ni5bh06xw"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JfmoAGKQ06xy",
    "tags": [
     "parameters"
    ],
    "ExecuteTime": {
     "end_time": "2025-03-16T17:47:06.402819Z",
     "start_time": "2025-03-16T17:47:06.398675Z"
    }
   },
   "source": [
    "#Parameters\n",
    "install_ta_lib_binary = False\n",
    "install_deps = False"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaTQWubyR6ZK",
    "outputId": "85da38aa-b1ca-4a96-b632-fea875a1889f",
    "ExecuteTime": {
     "end_time": "2025-03-16T17:47:06.456666Z",
     "start_time": "2025-03-16T17:47:06.454329Z"
    }
   },
   "source": [
    "# If we are using this in collab, then connection_string will by default be DEFAULT and when trigger manually or by paper mill, it will be set using\n",
    "# parameter injection\n",
    "if install_ta_lib_binary:\n",
    "    print(\"Installing ta lib binary\")\n",
    "    !wget https://github.com/ta-lib/ta-lib/releases/download/v0.6.3/ta-lib_0.6.3_amd64.deb\n",
    "    !dpkg -i ta-lib_0.6.3_amd64.deb\n",
    "\n",
    "if install_deps:\n",
    "    print(\"Install dependencies...\")\n",
    "    !pip install TA-Lib pandas-ta ocifs pyarrow pandas numpy==1.26.4 tables websockets sqlalchemy --quiet\n"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:47:06.464273Z",
     "start_time": "2025-03-16T17:47:06.460642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "\n",
    "oci_config = os.environ.get(\"OCI_CONFIG\")\n",
    "oci_key_content = os.environ.get(\"OCI_KEY\")\n",
    "oci_bucket = os.environ.get(\"OCI_BUCKET\")\n",
    "\n",
    "if oci_config is None or oci_key_content is None or oci_bucket is None:\n",
    "    raise KeyError(\"Missing OCI config\")\n",
    "\n",
    "OCI_PRIVATE_KEY_PATH = \"./key.pem\"\n",
    "with open(OCI_PRIVATE_KEY_PATH, \"w\") as key_file:\n",
    "    key_file.write(oci_key_content)\n",
    "    OCI_PRIVATE_KEY_PATH = key_file.name  # Full URL\n",
    "\n",
    "OCI_CONFIG_PATH = \"./config\"\n",
    "with open(OCI_CONFIG_PATH, \"w\") as config_file:\n",
    "    oci_config += f'\\nkey_file={OCI_PRIVATE_KEY_PATH}'\n",
    "    config_file.write(oci_config)\n",
    "    OCI_CONFIG_PATH = config_file.name\n",
    "\n",
    "storage_options = {\"config\": OCI_CONFIG_PATH}\n",
    "print(\"OCI FS Configured\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCI FS Configured\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5eo3XghP06xz",
    "ExecuteTime": {
     "end_time": "2025-03-16T17:47:06.490537Z",
     "start_time": "2025-03-16T17:47:06.488370Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import json\n",
    "from logging import log\n",
    "from random import choices\n",
    "from string import ascii_letters, digits\n",
    "from typing import Any\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import requests\n",
    "from websockets import ConnectionClosed\n",
    "from websockets import Origin\n",
    "from websockets.asyncio.client import connect, ClientConnection"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_koJxoQ06x0",
    "ExecuteTime": {
     "end_time": "2025-03-16T10:34:37.806432Z",
     "start_time": "2025-03-16T10:34:34.967074Z"
    }
   },
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_D6nrPUQ06x0",
    "ExecuteTime": {
     "end_time": "2025-03-16T17:47:06.555453Z",
     "start_time": "2025-03-16T17:47:06.539441Z"
    }
   },
   "source": [
    "_MESSAGE_PREFIX = \"~m~\"\n",
    "\n",
    "\n",
    "def chunk_list(lst: list[str], chunk_size: int):\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "async def download(tickers: list[str]):\n",
    "    all_quotes = {}\n",
    "    all_bars = {}\n",
    "    async  for quotes, bars in fetch_bulk(tickers):\n",
    "        all_quotes.update(quotes)\n",
    "        all_bars.update(bars)\n",
    "\n",
    "    return all_quotes, all_bars\n",
    "\n",
    "\n",
    "def to_quote_df(quotes: dict[str, dict]):\n",
    "    print(\"Generating Quote DataFrames...\")\n",
    "    quote_df = pd.DataFrame(quotes.values())\n",
    "    quote_df['ticker'] = quote_df['pro_name']\n",
    "    quote_df = quote_df.set_index(['ticker'])\n",
    "    print(\"Generated Quote DataFrames...\")\n",
    "    return quote_df\n",
    "\n",
    "\n",
    "def to_bars_df(bars: dict[str, list[list]]):\n",
    "    required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "    def process_bar(bar):\n",
    "        b_df = pd.DataFrame(bar)\n",
    "        # Add column names dynamically (truncate to available data)\n",
    "        b_df.columns = required_columns[:b_df.shape[1]]\n",
    "        # Ensure all required columns are present\n",
    "        for col in required_columns:\n",
    "            if col not in b_df.columns:\n",
    "                b_df[col] = np.nan  # Fill missing columns with NaN\n",
    "\n",
    "        b_df['timestamp'] = pd.to_datetime(b_df['timestamp'], unit='s')\n",
    "        b_df['timestamp'] = b_df['timestamp'].dt.floor('D')\n",
    "        b_df.set_index(['timestamp'], inplace=True)\n",
    "        return b_df\n",
    "\n",
    "    print(\"Generating Bar DataFrames...\")\n",
    "    v = {k: process_bar(bar) for k, bar in bars.items()}\n",
    "    print(\"Generated Bar DataFrames...\")\n",
    "    return v\n",
    "\n",
    "\n",
    "async def fetch_bulk(tickers: list[str]):\n",
    "    main_chunk = chunk_list(tickers, 500)\n",
    "    failed_chunks = []\n",
    "    for idx, chunk in enumerate(main_chunk):\n",
    "        print(f\"Started: {idx + 1}/{len(main_chunk)}\")\n",
    "        sub_chunks = chunk_list(chunk, 100)\n",
    "        tasks = [asyncio.create_task(fetch_data(chunked_symbols)) for chunked_symbols in sub_chunks]\n",
    "        chunk_result = await asyncio.gather(*tasks)\n",
    "        for chunked_symbols, result in zip(sub_chunks, chunk_result):\n",
    "            if result is None:\n",
    "                print(\"Failed chunks: \", chunked_symbols)\n",
    "                failed_chunks = failed_chunks + chunked_symbols\n",
    "                continue\n",
    "\n",
    "            yield result\n",
    "\n",
    "        print(f\"Completed: {idx + 1}/{len(main_chunk)}\")\n",
    "\n",
    "\n",
    "async def fetch_data(ticker: list[str], mode: Literal[\"quote\", \"bar\", \"all\"] = \"all\"):\n",
    "    if len(ticker) == 0:\n",
    "        return {}, {}\n",
    "\n",
    "    data = {}\n",
    "    complete = False\n",
    "    last_message = None\n",
    "    async with (connect_to_server() as socket):\n",
    "        try:\n",
    "            await _init(socket, ticker, data, mode)\n",
    "            async  for message in socket:\n",
    "                last_message = message\n",
    "                complete = await _process_data(socket, ticker, message, data)\n",
    "                if complete:\n",
    "                    break\n",
    "            await _end(socket)\n",
    "        except ConnectionClosed as e:\n",
    "            if not complete:\n",
    "                print(\"Failed\", last_message)\n",
    "                log.error(\"Connection Closed\", e)\n",
    "\n",
    "    if complete:\n",
    "        return data.get(\"quotes\", {}), data.get(\"bars\", {})\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def connect_to_server():\n",
    "    url = \"wss://data-wdc.tradingview.com/socket.io/websocket?type=chart\"\n",
    "    origin = \"https://in.tradingview.com\"\n",
    "    return connect(url, origin=Origin(origin), max_size=None, ping_timeout=60)\n",
    "\n",
    "\n",
    "def _encode(data: dict[str, Any] | list[dict[str, Any]] | str) -> str:\n",
    "    encoded_message = \"\"\n",
    "    if isinstance(data, str):\n",
    "        encoded_message += f\"{_MESSAGE_PREFIX}{len(data)}{_MESSAGE_PREFIX}{data}\"\n",
    "        return encoded_message\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "\n",
    "    for item in data:\n",
    "        stringified = json.dumps(item) if item is not None else \"\"\n",
    "        encoded_message += f\"{_MESSAGE_PREFIX}{len(stringified)}{_MESSAGE_PREFIX}{stringified}\"\n",
    "\n",
    "    return encoded_message\n",
    "\n",
    "\n",
    "async def _decode(socket: ClientConnection, msg: str) -> list[dict[str, Any]]:\n",
    "    decoded_messages = []\n",
    "    while msg.startswith(_MESSAGE_PREFIX):\n",
    "        msg = msg[len(_MESSAGE_PREFIX):]\n",
    "        separator_index = msg.find(_MESSAGE_PREFIX)\n",
    "        length = int(msg[:separator_index])\n",
    "        decoded_messages.append(\n",
    "            msg[separator_index + len(_MESSAGE_PREFIX):separator_index + len(_MESSAGE_PREFIX) + length])\n",
    "        msg = msg[separator_index + len(_MESSAGE_PREFIX) + length:]\n",
    "\n",
    "    events = []\n",
    "    for m in decoded_messages:\n",
    "        if m.startswith(\"~h~\"):\n",
    "            await _send(socket, m)\n",
    "        if m.startswith(\"{\"):\n",
    "            events.append(json.loads(m))\n",
    "    return events\n",
    "\n",
    "\n",
    "async def _init(socket: ClientConnection, tickers: list[str], data: dict[str, Any],\n",
    "                mode: Literal[\"quote\", \"bar\", \"all\"] = \"all\"):\n",
    "    qs_session = _gen_session_id(\"qs\")\n",
    "    cs_session = _gen_session_id(\"cs\")\n",
    "    keys = {f\"sds_sym_{i + 1}\": {\"t\": tickers[i], \"i\": i + 1} for i in range(len(tickers))}\n",
    "\n",
    "    # Store the key of the symbol that is completed\n",
    "    data['quotes'] = {}\n",
    "    data['bars'] = {}\n",
    "    data['bar_completed'] = 0\n",
    "    data['bar_started'] = []\n",
    "    data['quote_completed'] = 0\n",
    "    data['qs'] = qs_session\n",
    "    data['cs'] = cs_session\n",
    "    data['keys'] = keys\n",
    "\n",
    "    await _send(socket, {\"m\": \"set_auth_token\", \"p\": [\"unauthorized_user_token\"]})\n",
    "    await _send(socket, {\"m\": \"set_locale\", \"p\": [\"en\", \"IN\"]})\n",
    "\n",
    "    if mode == \"quote\":\n",
    "        data['bar_completed'] = len(tickers)\n",
    "\n",
    "    if mode == \"bar\":\n",
    "        data['quote_completed'] = len(tickers)\n",
    "\n",
    "    if mode == \"all\" or mode == \"quote\":\n",
    "        await _send(socket, {\"m\": \"quote_create_session\", \"p\": [qs_session]})\n",
    "        await _send(socket, {\"m\": \"quote_add_symbols\", \"p\": [qs_session, *tickers]})\n",
    "    if mode == \"all\" or mode == \"bar\":\n",
    "        await _send(socket, {\"m\": \"chart_create_session\", \"p\": [cs_session, \"\"]})\n",
    "        await _send(socket, {\"m\": \"switch_timezone\", \"p\": [cs_session, \"Asia/Kolkata\"]})\n",
    "        resolve_request = []\n",
    "        for symbol_key in keys:\n",
    "            meta = keys[symbol_key]\n",
    "            ticker = meta['t']\n",
    "            p = json.dumps({\"adjustment\": \"splits\", \"currency-id\": \"INR\", \"symbol\": ticker})\n",
    "            request = {\"m\": \"resolve_symbol\", \"p\": [cs_session, symbol_key, f'={p}']}\n",
    "            resolve_request.append(request)\n",
    "        await _send(socket, resolve_request)\n",
    "\n",
    "\n",
    "async def _end(socket: ClientConnection):\n",
    "    await socket.close()\n",
    "\n",
    "\n",
    "async def _send(socket: ClientConnection, data: str | dict[str, Any] | list[dict[str, Any]]):\n",
    "    message = _encode(data)\n",
    "    await socket.send(message)\n",
    "\n",
    "\n",
    "def _gen_session_id(prefix: str):\n",
    "    characters = ascii_letters + digits  # A-Z, a-z, 0-9\n",
    "    random_string = ''.join(choices(characters, k=12))\n",
    "    return f\"{prefix}_{random_string}\"\n",
    "\n",
    "\n",
    "async def _process_data(socket: ClientConnection, tickers: list[str], message: str | bytes, data: dict[str, Any]):\n",
    "    events = await _decode(socket, message)\n",
    "\n",
    "    for event in events:\n",
    "        event_type = event.get(\"m\")\n",
    "        if event_type == \"qsd\":\n",
    "            data['quotes'] = _on_qsd_event(event, data)\n",
    "        if event_type == \"quote_completed\":\n",
    "            completed_count = data['quote_completed']\n",
    "            data[\"quote_completed\"] = completed_count + 1\n",
    "        if event_type == \"symbol_resolved\":\n",
    "            await _on_symbol_resolved(socket, data)\n",
    "        if event_type == \"timescale_update\":\n",
    "            await _on_timescale_update(event, data)\n",
    "        if event_type == \"series_completed\":\n",
    "            await _on_series_completed(socket, tickers, data)\n",
    "\n",
    "    return data.get(\"quote_completed\", 0) == len(tickers) and data.get(\"bar_completed\", 0) == len(tickers)\n",
    "\n",
    "\n",
    "def _on_qsd_event(event: dict[str, Any], data: dict[str, Any]) -> dict[str, Any]:\n",
    "    quotes = data.get(\"quotes\", {})\n",
    "\n",
    "    q: dict = event.get(\"p\")[1]\n",
    "    ticker = q.get(\"n\")\n",
    "\n",
    "    if q.get(\"v\") is None:\n",
    "        return quotes\n",
    "\n",
    "    # Update Quote\n",
    "    ticker_quote = quotes.get(ticker, {})\n",
    "    q_data: dict = q.get(\"v\")\n",
    "    quotes[ticker] = ticker_quote | q_data\n",
    "\n",
    "    return quotes\n",
    "\n",
    "\n",
    "async def _on_symbol_resolved(socket: ClientConnection, data: dict[str, Any]):\n",
    "    symbol_resolve_count = data.get(\"symbol_resolve_count\", 0)\n",
    "    symbol_resolve_count = symbol_resolve_count + 1\n",
    "    data[\"symbol_resolve_count\"] = symbol_resolve_count\n",
    "\n",
    "    keys = data['keys']\n",
    "    ticker_count = len(keys.keys())\n",
    "    if symbol_resolve_count != ticker_count:\n",
    "        # All symbol not yet resolved\n",
    "        return\n",
    "\n",
    "    bar_started = data['bar_started']\n",
    "    to_start = list(set(keys.keys()) - set(bar_started))\n",
    "    if len(to_start) == 0:\n",
    "        return\n",
    "\n",
    "    # Start with the first pending\n",
    "    cs = data[\"cs\"]\n",
    "    symbol_key = to_start[0]\n",
    "    series_id = f\"s{keys[symbol_key]['i']}\"\n",
    "\n",
    "    # Request data\n",
    "    await _send(socket, {\"m\": \"create_series\", \"p\": [cs, \"sds_1\", series_id, symbol_key, \"1D\", 5500]})\n",
    "    bar_started.append(symbol_key)\n",
    "\n",
    "\n",
    "async def _on_timescale_update(event: dict[str, Any], data: dict[str, Any]):\n",
    "    p: dict[str, Any] = event.get(\"p\")[1]\n",
    "    series = p.get('sds_1')\n",
    "    if series is None or series.get(\"s\") is None:\n",
    "        print(\"Series is missing\", event)\n",
    "        return\n",
    "\n",
    "    # Day Data\n",
    "    d = list(map(lambda s: s['v'], series.get(\"s\")))\n",
    "\n",
    "    keys = data['keys']\n",
    "    bar_started = data['bar_started']\n",
    "    bars = data['bars']\n",
    "\n",
    "    # Mark the bars to loaded\n",
    "    last_bar_key = bar_started[-1]\n",
    "    ticker = keys[last_bar_key]['t']\n",
    "\n",
    "    bar = bars.get(ticker, [])\n",
    "    bars[ticker] = d + bar\n",
    "\n",
    "\n",
    "async def _on_series_completed(socket: ClientConnection, ticker: list[str], data: dict[str, Any]):\n",
    "    cs = data[\"cs\"]\n",
    "    keys = data['keys']\n",
    "    bar_started = data['bar_started']\n",
    "    bar_completed = data['bar_completed'] + 1\n",
    "    data['bar_completed'] = bar_completed\n",
    "\n",
    "    if bar_completed == len(ticker):\n",
    "        return\n",
    "\n",
    "    pending = list(set(keys.keys()) - set(bar_started))\n",
    "    if len(pending) == 0:\n",
    "        return\n",
    "\n",
    "    symbol_key = pending[0]\n",
    "    meta = keys[symbol_key]\n",
    "    series_id = f\"s{meta['i']}\"\n",
    "\n",
    "    await _send(socket, {\"m\": \"modify_series\", \"p\": [cs, \"sds_1\", series_id, symbol_key, \"1D\", \"\"]})\n",
    "    bar_started.append(symbol_key)"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TtD3cqpOfCK-",
    "ExecuteTime": {
     "end_time": "2025-03-16T17:47:06.560137Z",
     "start_time": "2025-03-16T17:47:06.557739Z"
    }
   },
   "source": [
    "def get_all_symbols(market: str):\n",
    "    indexes = {\n",
    "        \"india\": [\n",
    "            \"NSE:NIFTY\", \"NSE:NIFTYJR\", \"NSE:CNX500\", \"NSE:BANKNIFTY\", \"NSE:CNXFINANCE\", \"NSE:CNXIT\",\n",
    "            \"NSE:CNXAUTO\", \"NSE:CNXPHARMA\", \"NSE:CNXPSUBANK\", \"NSE:CNXMETAL\", \"NSE:CNXFMCG\", \"NSE:CNXREALTY\",\n",
    "            \"NSE:CNXMEDIA\", \"NSE:CNXINFRA\", \"NSE:NIFTYPVTBANK\", \"NSE:NIFTY_OIL_AND_GAS\", \"NSE:NIFTY_HEALTHCARE\",\n",
    "            \"NSE:NIFTY_CONSR_DURBL\", \"NSE:CNX200\", \"NSE:NIFTY_MID_SELECT\",\n",
    "            \"NSE:CNXSMALLCAP\", \"NSE:CNXMIDCAP\", \"NSE:CNXENERGY\", \"NSE:NIFTYMIDCAP50\", \"NSE:NIFTYSMLCAP250\",\n",
    "            \"NSE:CNXPSE\", \"NSE:NIFTYMIDSML400\", \"NSE:NIFTYMIDCAP150\", \"NSE:CNXCONSUMPTION\", \"NSE:CNXCOMMODITIES\",\n",
    "            \"NSE:NIFTY_MICROCAP250\", \"NSE:CPSE\", \"NSE:CNXSERVICE\", \"NSE:CNXMNC\", \"NSE:CNX100\", \"NSE:NIFTYALPHA50\",\n",
    "            \"NSE:NIFTY_TOTAL_MKT\", \"NSE:NIFTY_INDIA_MFG\",\n",
    "            \"NSE:NIFTY_IND_DIGITAL\", \"NSE:NIFTY_LARGEMID250\", \"BSE:SENSEX\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    url = f\"https://scanner.tradingview.com/{market}/scan\"\n",
    "    payload = {\n",
    "        \"columns\": [],\n",
    "        \"filter\": [\n",
    "            {\n",
    "                \"left\": \"exchange\",\n",
    "                \"operation\": \"in_range\",\n",
    "                \"right\": [\"NSE\"]\n",
    "            }\n",
    "        ],\n",
    "        \"sort\": {\n",
    "            \"sortBy\": \"market_cap_basic\",\n",
    "            \"sortOrder\": \"desc\"\n",
    "        },\n",
    "    }\n",
    "    headers = {'Content-Type': 'text/plain'}\n",
    "\n",
    "    r = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload))\n",
    "    r.raise_for_status()\n",
    "    data = r.json()['data']  # [{'s': 'NYSE:HKD', 'd': []}, {'s': 'NASDAQ:ALTY', 'd': []}...]\n",
    "    return indexes[market] + [i['s'] for i in data]\n"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpkUJPML06x1",
    "outputId": "122378c2-9ef5-405a-c8d1-445d384d6736",
    "ExecuteTime": {
     "end_time": "2025-03-16T17:47:07.515770Z",
     "start_time": "2025-03-16T17:47:06.584075Z"
    }
   },
   "source": [
    "t = get_all_symbols(\"india\")\n",
    "print(\"Ticker fetched\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker fetched\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDl-oyiH06x1",
    "outputId": "266f9f2f-80e1-47d1-d62e-e06f9c6e2d98",
    "ExecuteTime": {
     "end_time": "2025-03-16T17:52:05.330675Z",
     "start_time": "2025-03-16T17:47:07.577311Z"
    }
   },
   "source": [
    "quotes, bars = await download(t)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 1/2\n",
      "Completed: 1/2\n",
      "Started: 2/2\n",
      "Completed: 2/2\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7GtHPtXDJVpF",
    "outputId": "572864ea-fe4f-414a-fdda-238b645d7ff2",
    "ExecuteTime": {
     "end_time": "2025-03-16T17:52:05.372101Z",
     "start_time": "2025-03-16T17:52:05.369468Z"
    }
   },
   "source": [
    "len(quotes.keys())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwTsWz2yfN4q",
    "outputId": "e0fad5ec-96ae-438a-e268-81a562aa9837",
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:35.279422Z",
     "start_time": "2025-03-16T18:21:35.009556Z"
    }
   },
   "source": [
    "q = to_quote_df(quotes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Quote DataFrames...\n",
      "Generated Quote DataFrames...\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JX9-UdpufT5H",
    "outputId": "8016aff8-6f7d-492a-a141-1493d69f08d6",
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:37.382970Z",
     "start_time": "2025-03-16T18:21:35.861179Z"
    }
   },
   "source": [
    "daily_candles = to_bars_df(bars)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bar DataFrames...\n",
      "Generated Bar DataFrames...\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yMM2b7_fWcx"
   },
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZhAxmlDfa9V"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "43pB13hgfXSq",
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:39.548988Z",
     "start_time": "2025-03-16T18:21:39.539389Z"
    }
   },
   "source": [
    "# Function to process each row\n",
    "def calculate_surprise(value):\n",
    "    if isinstance(value, list):  # Only process if the value is a list of dicts\n",
    "        # Create a temporary DataFrame\n",
    "        temp_df = pd.DataFrame(value)\n",
    "        # Ensure the required keys are present\n",
    "        if 'Estimate' not in temp_df.columns:\n",
    "            temp_df['Estimate'] = None\n",
    "        if 'Actual' not in temp_df.columns:\n",
    "            temp_df['Actual'] = None\n",
    "        # Add the Surprise column\n",
    "        temp_df['Surprise'] = (temp_df['Actual'] - temp_df['Estimate']) / temp_df['Estimate'] * 100\n",
    "        # Convert back to a list of dicts\n",
    "        temp_df = temp_df.where(pd.notnull(temp_df), None)\n",
    "        temp_df = temp_df.replace([np.nan, np.inf, -np.inf], None)\n",
    "\n",
    "        return temp_df.to_dict(orient='records')\n",
    "    elif pd.isna(value):  # Replace NaN with an empty list\n",
    "        return []\n",
    "    else:\n",
    "        return value  # Return as-is for other types\n",
    "\n",
    "\n",
    "def get_surprise_df(s: pd.Series, n: int, prefix: str, index: pd.Series):\n",
    "    # Extract the latest n Surprise values and create individual columns\n",
    "    def extract_latest_surprise(value):\n",
    "        if isinstance(value, list):\n",
    "            # Extract the Surprise values, reverse the order for latest first\n",
    "            surprise_values = [d.get('Surprise', None) for d in value if d.get(\"IsReported\", False)][-n:]\n",
    "            # Fill the list to ensure exactly `n` values\n",
    "            return list(reversed(surprise_values + [None] * (n - len(surprise_values))))\n",
    "        return [None] * n  # Return empty if not a list\n",
    "\n",
    "    surprise_columns = [f\"{prefix}_{i}\" for i in range(n)]\n",
    "    surprises_values = s.map(lambda x: extract_latest_surprise(x)).tolist()\n",
    "    return pd.DataFrame(surprises_values, index=index, columns=surprise_columns, dtype='float32')\n",
    "\n",
    "\n",
    "def flatten_list(s: pd.Series, n: int, prefix: str):\n",
    "    cols = {}\n",
    "    s_normalized = s.apply(lambda x: x if isinstance(x, list) else []).astype(object)\n",
    "    cols[f'{prefix}_h'] = s_normalized\n",
    "\n",
    "    for i in range(n):\n",
    "        col = f'{prefix}_{i}'\n",
    "        cols[col] = s_normalized.map(lambda x: x[i] if i < len(x) else None).astype(float)\n",
    "\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def growth(d: pd.DataFrame, comp: str, n: int, name: str, period: int = 1):\n",
    "    cols = {}\n",
    "    for i in range(n - period):\n",
    "        curr_col = f\"{comp}_{i}\"\n",
    "        prev_col = f\"{comp}_{i + period}\"\n",
    "        col = f\"{name}_{i}\"\n",
    "        cols[col] = ((d[curr_col] - d[prev_col]) / d[prev_col] * 100).astype('float32')\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def forecast_growth(d: pd.DataFrame, l_rep: str, comp: str, n: int, name: str, loopback: int = 1):\n",
    "    cols = {}\n",
    "    for i in range(min(n - loopback, 4)):\n",
    "        curr_col = f\"{comp}_{i}\"\n",
    "        prev_col = l_rep if i == 0 else f\"{comp}_{i - 1}\"\n",
    "        col = f\"{name}_{i}\"\n",
    "        cols[col] = ((d[curr_col] - d[prev_col]) / d[prev_col] * 100).astype('float32')\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def growth_avg(n: int, comp: str, name: str):\n",
    "    cols = {}\n",
    "    for i in range(2, min(4, n)):\n",
    "        needed_cols = [f\"{comp}_{r}\" for r in range(i)]\n",
    "        col = f\"{name}_{i}\"\n",
    "        cols[col] = df[needed_cols].mean(axis=1, skipna=True)\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def to_datetime(series: pd.Series, unit='s', utc=True):\n",
    "    return pd.to_datetime(series.astype('Int64'), unit=unit)\n",
    "\n",
    "\n",
    "def to_weekly_candles(d: pd.DataFrame):\n",
    "    # TODO: Try with .resample('W-MON', label='left', closed='left')\n",
    "\n",
    "    d = d.copy()\n",
    "    # Step 1: Adjust the timestamp index to the start of the week (Monday 12:00 AM UTC)\n",
    "    d[\"Week_Start\"] = d.index.to_period(\"W\").start_time\n",
    "\n",
    "    # Step 2: Group by the week start\n",
    "    w: pd.DataFrame = (d.groupby(\"Week_Start\").agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }).reset_index())\n",
    "\n",
    "    w = w.rename(columns={'Week_Start': 'timestamp'}).set_index('timestamp')\n",
    "    return w\n",
    "\n",
    "\n",
    "def to_monthly_candles(d: pd.DataFrame):\n",
    "    d = d.copy()\n",
    "    # Step 1: Adjust the timestamp index to the start of the week (Monday 12:00 AM UTC)\n",
    "    d[\"Month_Start\"] = d.index.to_period(\"M\").start_time\n",
    "\n",
    "    # Step 2: Group by the week start\n",
    "    m = (d.groupby(\"Month_Start\").agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }).reset_index())\n",
    "\n",
    "    m = m.rename(columns={'Month_Start': 'timestamp'}).set_index('timestamp')\n",
    "    return m\n",
    "\n",
    "\n",
    "def to_yearly_candles(m: pd.DataFrame):\n",
    "    return m.resample('YS').agg({\n",
    "        'open': 'first',  # First Open in the year\n",
    "        'high': 'max',  # Maximum High in the year\n",
    "        'low': 'min',  # Minimum Low in the year\n",
    "        'close': 'last',  # Last Close in the year\n",
    "        'volume': 'sum'  # Sum of Volume in the year\n",
    "    })"
   ],
   "outputs": [],
   "execution_count": 138
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZnzWyaBffRE"
   },
   "source": [
    "## Fundamental"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:39.987196Z",
     "start_time": "2025-03-16T18:21:39.983982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fq_count = 12\n",
    "fy_count = 4\n",
    "today = pd.Timestamp.today()\n",
    "df = pd.DataFrame()\n",
    "df['ticker'] = q.index.astype(str)\n",
    "df = df.set_index(['ticker'])"
   ],
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### General"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:40.400925Z",
     "start_time": "2025-03-16T18:21:40.392385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['isin'] = q['isin'].astype(str)\n",
    "df['name'] = q['short_name'].astype(str)\n",
    "df['is_primary_listing'] = q['is-primary-listing'].astype(bool)\n",
    "df['logo'] = q['logoid'].astype(str)\n",
    "df['description'] = q['description'].astype(str)\n",
    "df['type'] = q['type'].astype('category')\n",
    "df['exchange'] = (q['source-id'].fillna(q['exchange']).fillna(q['exchange-listed']).astype('category'))\n",
    "df['exchange_logo'] = (q['source-logoid'].astype('category'))\n",
    "df['exchange_logo'] = df[\"exchange_logo\"].where(\n",
    "    df[\"exchange_logo\"].notna(),\n",
    "    df.groupby(\"exchange\", observed=True)[\"exchange_logo\"].transform(\"first\")\n",
    ")\n",
    "df['timezone'] = q['timezone'].astype('category')\n",
    "df['currency'] = (q['currency-id']\n",
    "                  .fillna(q['currency_id'])\n",
    "                  .fillna(q['currency_code'])\n",
    "                  .fillna(q['currency_fund'])\n",
    "                  .fillna(q['currency'])\n",
    "                  .astype('category'))\n",
    "df['currency_logo'] = q['currency-logoid']\n",
    "df['fundamental_currency'] = (q['fundamental_currency_code'].fillna(df['currency']).astype('category'))\n",
    "df['subsessions'] = q['subsessions']\n",
    "df['session_holidays'] = q['session_holidays']"
   ],
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fiscal"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:40.817195Z",
     "start_time": "2025-03-16T18:21:40.809629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['fiscal_period_fy'] = q['fiscal_period_fy'].astype('category')\n",
    "df['fiscal_period_end_fy'] = to_datetime(q['fiscal_period_end_fy'])\n",
    "df['fiscal_period_fq'] = q['fiscal_period_fq'].astype('category')\n",
    "df['fiscal_period_end_fq'] = to_datetime(q['fiscal_period_end_fq'])\n",
    "df['fiscal_period_fy_h'] = q['fiscal_period_fy_h']\n",
    "df['fiscal_period_end_fy_h'] = q['fiscal_period_end_fy_h']\n",
    "df['fiscal_period_fq_h'] = q['fiscal_period_fq_h']\n",
    "df['fiscal_period_end_fq_h'] = q['fiscal_period_end_fq_h']"
   ],
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Company"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:41.304230Z",
     "start_time": "2025-03-16T18:21:41.294351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['fundamental_data'] = q['fundamental_data']\n",
    "df['sector'] = q['sector'].astype('category')\n",
    "df['group'] = q['group'].astype('category')\n",
    "df['industry'] = q['industry'].astype('category')\n",
    "df['sub_industry'] = None\n",
    "df['sub_industry'] = df[\"sub_industry\"].astype('category')\n",
    "df['logo'] = q['logoid'].astype(str)\n",
    "df['ceo'] = q['ceo'].astype(str)\n",
    "df['website'] = q['web_site_url'].astype(str)\n",
    "df['country'] = q['country'].astype('category')\n",
    "df['location'] = q['location'].astype(str)\n",
    "df['country_code'] = q['country_code'].astype('category')\n",
    "df['employees'] = q['number_of_employees'].astype('Int64')\n",
    "df['business_description'] = q['business_description'].astype(str)\n",
    "# TODO: Ensure we have the correct data\n",
    "# df['ipo_date'] = to_datetime(q['first_bar_time_1d'])\n",
    "df['most_recent_split'] = to_datetime(q['split_last_date'])\n",
    "df['mcap'] = q['market_cap_basic'].astype(float)\n",
    "df['shares_float'] = q['float_shares_outstanding'].astype(float)\n",
    "df['total_shares_outstanding'] = q['total_shares_outstanding'].astype(float)"
   ],
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Revenue"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:43.239442Z",
     "start_time": "2025-03-16T18:21:41.726690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['price_revenue_ttm'] = q['price_revenue_ttm']\n",
    "df['revenue_action_fq_h'] = q['revenues_fq_h'].map(calculate_surprise)\n",
    "df['revenue_action_fy_h'] = q['revenues_fy_h'].map(calculate_surprise)\n",
    "# Add Surprise FQ\n",
    "df = pd.concat([df, get_surprise_df(df['revenue_action_fq_h'], fq_count, prefix=\"revenue_surprise_fq\", index=df.index)],\n",
    "               axis=1)\n",
    "# Add Surprise FY\n",
    "df = pd.concat([df, get_surprise_df(df['revenue_action_fy_h'], fy_count, prefix=\"revenue_surprise_fy\", index=df.index)],\n",
    "               axis=1)\n",
    "# Reported Revenue FQ\n",
    "df = pd.concat([df, flatten_list(q['revenue_fq_h'], fq_count, prefix=\"revenue_fq\")], axis=1)\n",
    "# Reported Revenue FY\n",
    "df = pd.concat([df, flatten_list(q['revenue_fy_h'], fy_count, prefix=\"revenue_fy\")], axis=1)\n",
    "# Forecasted Revenue FQ\n",
    "df = pd.concat([df, flatten_list(q['revenue_forecast_fq_h'], fq_count, prefix=\"revenue_forecast_fq\")], axis=1)\n",
    "# Forecasted Revenue FY\n",
    "df = pd.concat([df, flatten_list(q['revenue_forecast_fy_h'], fy_count, prefix=\"revenue_forecast_fy\")], axis=1)\n",
    "# Reported Revenue Growth FQ\n",
    "df = pd.concat([df, growth(df, comp='revenue_fq', n=fq_count, name='revenue_growth_fq')], axis=1)\n",
    "# Reported Revenue Growth YOY FQ\n",
    "df = pd.concat([df, growth(df, comp='revenue_fq', n=fq_count, name='revenue_growth_yoy_fq', period=4)], axis=1)\n",
    "# Reported Revenue Growth FY\n",
    "df = pd.concat([df, growth(df, comp='revenue_fy', n=fy_count, name='revenue_growth_fy')], axis=1)\n",
    "# Report Average Revenue Growth FQ\n",
    "df = pd.concat([df, growth_avg(n=fq_count, comp='revenue_growth_fq', name='revenue_avg_growth_fq')], axis=1)\n",
    "# Report Average Revenue Growth FY\n",
    "df = pd.concat([df, growth_avg(n=fy_count, comp='revenue_growth_fy', name='revenue_avg_growth_fy')], axis=1)\n",
    "# Forecast Revenue Growth FQ\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    forecast_growth(df, n=fq_count, l_rep='revenue_fq_0', comp='revenue_forecast_fq', name='revenue_forecast_growth_fq')\n",
    "], axis=1)\n",
    "# Forecast Revenue Growth FY\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    forecast_growth(df, n=fy_count, l_rep='revenue_fy_0', comp='revenue_forecast_fy', name='revenue_forecast_growth_fy')\n",
    "], axis=1)\n",
    "# Forecast Average Revenue Growth FQ\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    growth_avg(n=fq_count, comp='revenue_forecast_growth_fq', name='revenue_forecast_growth_avg_fq')\n",
    "], axis=1)\n",
    "# Forecast Average Revenue Growth FY\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    growth_avg(n=fy_count, comp='revenue_forecast_growth_fy', name='revenue_forecast_growth_avg_fy')\n",
    "], axis=1)"
   ],
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Earning"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:44.762089Z",
     "start_time": "2025-03-16T18:21:43.274201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#All Earning Release\n",
    "df['earnings_release_date_fq_h'] = q['earnings_release_date_fq_h']  #ABS Date\n",
    "df['earnings_release_date_fy_h'] = q['earnings_release_date_fy_h']  #ABD Date\n",
    "df['earnings_fiscal_period_fq_h'] = q['earnings_fiscal_period_fq_h']  # Period\n",
    "df['earnings_fiscal_period_fy_h'] = q['earnings_fiscal_period_fy_h']  # Period\n",
    "\n",
    "# Latest Earning Date\n",
    "df['earnings_release_date'] = to_datetime(q['earnings_release_date'])\n",
    "df['earnings_release_time'] = q['earnings_release_time'] == 1\n",
    "# Latest FQ Earning Release Date\n",
    "df['earnings_release_date_fq'] = to_datetime(q['earnings_release_date_fq'])\n",
    "df['earnings_release_time_fq'] = q['earnings_release_time_fq'] == 1\n",
    "# Latest FQ Earning Release Date\n",
    "df['earnings_release_date_fy'] = to_datetime(q['earnings_release_date_fy'])\n",
    "# Next Latest Earning Date\n",
    "df['earnings_release_next_date'] = to_datetime(q['earnings_release_next_date'])\n",
    "df['earnings_release_next_time'] = q['earnings_release_next_time'] == 1\n",
    "# Next FQ Earning Release Date\n",
    "df['earnings_release_next_date_fq'] = to_datetime(q['earnings_release_next_date_fq'])\n",
    "df['earnings_release_next_time_fq'] = q['earnings_release_next_time_fq'] == 1\n",
    "# Next FY Earning Release Date\n",
    "df['earnings_release_next_date_fy'] = to_datetime(q['earnings_release_next_date_fy'])\n",
    "# Latest Earning Release Trading Date\n",
    "df['earnings_release_trading_date_fq'] = to_datetime(q['earnings_release_trading_date_fq'])\n",
    "df['earnings_release_trading_date_fy'] = to_datetime(q['earnings_release_trading_date_fy'])\n",
    "df['earnings_release_trading_date'] = df[['earnings_release_trading_date_fq', 'earnings_release_trading_date_fy']].max(\n",
    "    axis=1)\n",
    "# Latest Next Earning Release Trading Date\n",
    "df['earnings_release_next_trading_date_fq'] = to_datetime(q['earnings_release_next_trading_date_fq'])\n",
    "df['earnings_release_next_trading_date_fy'] = to_datetime(q['earnings_release_next_trading_date_fy'])\n",
    "df['earnings_release_next_trading_date'] = df[\n",
    "    ['earnings_release_next_trading_date_fq', 'earnings_release_next_trading_date_fy']].min(axis=1)\n",
    "\n",
    "df['earning_action_fq_h'] = q['earnings_fq_h'].map(calculate_surprise)\n",
    "df['earning_action_fy_h'] = q['earnings_fy_h'].map(calculate_surprise)\n",
    "df['earnings_per_share_ttm'] = q['earnings_per_share_ttm'].astype(float)\n",
    "\n",
    "# Add Surprise FQ\n",
    "df = pd.concat([df, get_surprise_df(df['earning_action_fq_h'], fq_count, prefix=\"earning_surprise_fq\", index=df.index)],\n",
    "               axis=1)\n",
    "# Add Surprise FY\n",
    "df = pd.concat([df, get_surprise_df(df['earning_action_fy_h'], fy_count, prefix=\"earning_surprise_fy\", index=df.index)],\n",
    "               axis=1)\n",
    "# Reported Earning FQ\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_diluted_fq_h'], fq_count, prefix=\"eps_fq\")], axis=1)\n",
    "# Reported Earning FY\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_diluted_fy_h'], fy_count, prefix=\"eps_fy\")], axis=1)\n",
    "# Reported Earning TTM\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_diluted_ttm_h'], fy_count, prefix=\"eps_ttm\")], axis=1)\n",
    "# Estimated Earning FQ\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_forecast_fq_h'], fq_count, prefix=\"eps_estimated_fq\")], axis=1)\n",
    "# Estimated Earning FY\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_forecast_fy_h'], fy_count, prefix=\"eps_estimated_fy\")], axis=1)\n",
    "# Reported Earning Growth FQ\n",
    "df = pd.concat([df, growth(df, comp='eps_fq', n=fq_count, name='eps_growth_fq')], axis=1)\n",
    "# Reported Earning Growth YOY FQ\n",
    "df = pd.concat([df, growth(df, comp='eps_fq', n=fq_count, name='eps_growth_yoy_fq', period=4)], axis=1)\n",
    "# Reported Earning Growth FY\n",
    "df = pd.concat([df, growth(df, comp='eps_fy', n=fy_count, name='eps_growth_fy')], axis=1)\n",
    "# Reported Earning Growth TTM\n",
    "df = pd.concat([df, growth(df, comp='eps_ttm', n=fy_count, name='eps_growth_ttm')], axis=1)\n",
    "# Reported Average Revenue Growth FQ\n",
    "df = pd.concat([df, growth_avg(n=fq_count, comp='eps_growth_fq', name='eps_avg_growth_fq')], axis=1)\n",
    "# Report Average Revenue Growth FY\n",
    "df = pd.concat([df, growth_avg(n=fy_count, comp='eps_growth_fy', name='eps_avg_growth_fy')], axis=1)\n",
    "# Estimated Earning Growth FQ\n",
    "df = pd.concat([df, forecast_growth(df, n=fq_count, l_rep='eps_fq_0', comp='eps_estimated_fq',\n",
    "                                    name='eps_estimated_growth_fq')], axis=1)\n",
    "# Estimated Earning Growth FY\n",
    "df = pd.concat([df, forecast_growth(df, n=fy_count, l_rep='eps_fy_0', comp='eps_estimated_fy',\n",
    "                                    name='eps_estimated_growth_fy')], axis=1)\n",
    "# Estimated Earning Growth Average FQ\n",
    "df = pd.concat([df, growth_avg(n=fq_count, comp='eps_estimated_growth_fq', name='eps_estimated_growth_avg_fq')], axis=1)\n",
    "# Estimated Earning Growth Average FY\n",
    "df = pd.concat([df, growth_avg(n=fy_count, comp='eps_estimated_growth_fy', name='eps_estimated_growth_avg_fy')], axis=1)\n",
    "# Net Income TTM\n",
    "df = pd.concat([df, flatten_list(q['net_income_ttm_h'], fq_count, prefix=\"net_income_ttm\")], axis=1)\n",
    "# Net Income FQ\n",
    "df = pd.concat([df, flatten_list(q['net_income_fq_h'], fq_count, prefix=\"net_income_fq\")], axis=1)\n",
    "# Net Income FY\n",
    "df = pd.concat([df, flatten_list(q['net_income_fy_h'], fq_count, prefix=\"net_income_fy\")], axis=1)"
   ],
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Balance Sheet"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:44.784557Z",
     "start_time": "2025-03-16T18:21:44.765156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Total Asset FQ\n",
    "df = pd.concat([df, flatten_list(q['total_assets_fq_h'], fq_count, prefix=\"total_assets_fq\")], axis=1)\n",
    "\n",
    "# Total Liability FY\n",
    "df = pd.concat([df, flatten_list(q['total_liabilities_fq_h'], fq_count, prefix=\"total_liabilities_fq\")], axis=1)\n",
    "\n",
    "# Total Asset FY\n",
    "df = pd.concat([df, flatten_list(q['total_assets_fy_h'], fq_count, prefix=\"total_assets_fy\")], axis=1)\n",
    "\n",
    "# Total Liability FY\n",
    "df = pd.concat([df, flatten_list(q['total_liabilities_fy_h'], fq_count, prefix=\"total_liabilities_fy\")], axis=1)"
   ],
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dividen"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:44.823890Z",
     "start_time": "2025-03-16T18:21:44.808313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['dividend_amount'] = q['dividend_amount_recent'].astype(float)\n",
    "df['divided_ex_date'] = to_datetime(q['dividend_ex_date_recent'])\n",
    "df['divided_payment_date'] = to_datetime(q['dividend_payment_date_recent'])\n",
    "df['dividend_yield'] = q['dividends_yield_current'].astype('float16')\n",
    "df = pd.concat([df, flatten_list(q['dividends_yield_fy_h'], fq_count, prefix=\"dividends_yield_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['dividend_payout_ratio_fq_h'], fq_count, prefix=\"dividend_payout_ratio_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['dividend_payout_ratio_fy_h'], fq_count, prefix=\"dividend_payout_ratio_fy\")], axis=1)"
   ],
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extra"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:44.902921Z",
     "start_time": "2025-03-16T18:21:44.847237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['price_earnings_ttm'] = q['price_earnings_ttm']\n",
    "df['price_revenue_ttm'] = q['price_revenue_ttm']\n",
    "df['price_sales_ttm'] = q['price_sales_ttm']\n",
    "df['price_earnings_growth_ttm'] = q['price_earnings_growth_ttm']\n",
    "df['current_ratio'] = q['current_ratio']\n",
    "df['price_earnings_run_rate'] = q['lp'] / q['earnings_per_share_fq'] * 4\n",
    "df['forward_price_earnings'] = q['lp'] / q['earnings_per_share_forecast_fy']\n",
    "\n",
    "df = pd.concat([df, flatten_list(q['price_earnings_fy_h'], fq_count, prefix=\"price_earnings_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_earnings_fq_h'], fq_count, prefix=\"price_earnings_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['current_ratio_fq_h'], fq_count, prefix=\"current_ratio_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['current_ratio_fy_h'], fq_count, prefix=\"current_ratio_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['debt_to_equity_fq_h'], fq_count, prefix=\"debt_to_equity_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['debt_to_equity_fy_h'], fq_count, prefix=\"debt_to_equity_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_book_fq_h'], fq_count, prefix=\"price_book_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_book_fy_h'], fq_count, prefix=\"price_book_fy\")], axis=1)\n",
    "# TODO Fix: Causing Parquet file serialization error\n",
    "# df = pd.concat([df, flatten_list(q['price_sales_fq_h'], fq_count, prefix=\"price_sales_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_sales_fy_h'], fq_count, prefix=\"price_sales_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebit_fq_h'], fq_count, prefix=\"ebit_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebit_fy_h'], fy_count, prefix=\"ebit_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebitda_fq_h'], fq_count, prefix=\"ebitda_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebitda_fy_h'], fy_count, prefix=\"ebitda_fy\")], axis=1)"
   ],
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T10:33:57.256443Z",
     "start_time": "2025-03-16T10:33:57.253618Z"
    }
   },
   "cell_type": "markdown",
   "source": "### NSE Industry Classification"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:44.948412Z",
     "start_time": "2025-03-16T18:21:44.945844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_nse_industry_classification(df: pd.DataFrame):\n",
    "    # Download the nse industry\n",
    "    industry_df = pd.read_csv(f'oci://{oci_bucket}/nse_industry_symbols.csv', storage_options=storage_options)\n",
    "    industry_df['ticker'] = \"NSE:\" + industry_df[\"Symbol\"]\n",
    "    industry_df = industry_df.drop(columns=[\"Symbol\"])\n",
    "    industry_df = industry_df.rename(\n",
    "        columns={\"Sector\": \"sector\", \"Industry\": \"industry\", \"Basic Industry\": \"sub_industry\", \"Macro\": \"macro_sector\"})\n",
    "    industry_df = industry_df.set_index([\"ticker\"])\n",
    "\n",
    "    # Merge df1 and df2 on ticker index\n",
    "    merged = df.copy().merge(industry_df, left_index=True, right_index=True, how='left', suffixes=('', '_new'))\n",
    "\n",
    "    # Loop through columns in df2\n",
    "    for col in industry_df.columns:\n",
    "        new_col = f'{col}_new'\n",
    "        if col in df.columns:\n",
    "            # Overwrite only where df2 has non-null values\n",
    "            merged[col] = merged[new_col].combine_first(merged[col])\n",
    "            # Drop the extra merged column\n",
    "            merged.drop(columns=[new_col], inplace=True)\n",
    "        else:\n",
    "            # New column, already added by merge, nothing to do\n",
    "            pass\n",
    "\n",
    "    return merged\n"
   ],
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:46.461883Z",
     "start_time": "2025-03-16T18:21:45.290694Z"
    }
   },
   "cell_type": "code",
   "source": "df = merge_nse_industry_classification(df)",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Technical"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T11:28:52.977973Z",
     "start_time": "2025-03-16T11:28:52.807578Z"
    }
   },
   "cell_type": "markdown",
   "source": "### Technical Utils"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:49.550008Z",
     "start_time": "2025-03-16T18:21:49.547606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_latest(value, index: int = -1):\n",
    "    if value is None:\n",
    "        return np.nan\n",
    "    if isinstance(value, pd.Series):\n",
    "        if index < 0 and abs(index) > len(value):\n",
    "            return np.nan\n",
    "        if index >= 0 and index >= len(value):\n",
    "            return np.nan\n",
    "        return value.iloc[index]\n",
    "    return value"
   ],
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OHLCV"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:50.938733Z",
     "start_time": "2025-03-16T18:21:50.935904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ohlcv(candle: pd.DataFrame, name: str, shift: int = 0) -> dict[str, pd.Series]:\n",
    "    return {\n",
    "        f\"{name}_open\": candle.open if shift == 0 else candle.open.shift(shift),\n",
    "        f\"{name}_high\": candle.high if shift == 0 else candle.high.shift(shift),\n",
    "        f\"{name}_low\": candle.low if shift == 0 else candle.low.shift(shift),\n",
    "        f\"{name}_close\": candle.close if shift == 0 else candle.close.shift(shift),\n",
    "        f\"{name}_volume\": candle.volume if shift == 0 else candle.volume.shift(shift),\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Price Action"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### VWAP"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:52.365739Z",
     "start_time": "2025-03-16T18:21:52.363498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vwap(candle: pd.DataFrame, name: str) -> dict[str, pd.Series]:\n",
    "    v = (candle.high + candle.low + candle.close) / 3\n",
    "    away = (candle.close - v) / v * 100\n",
    "    return {\n",
    "        f'{name}_vwap': v,\n",
    "        f'away_from_{name}_vwap_pct': away,\n",
    "        f'price_above_{name}_vwap': candle.close > v,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Price Change"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:52.958224Z",
     "start_time": "2025-03-16T18:21:52.955913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def price_change_close(candle: pd.DataFrame, periods: list[int] | range, name: str) -> dict[str, pd.Series]:\n",
    "    return {\n",
    "        f\"price_change_pct_{i}{name}\": (candle.close.pct_change(periods=i, fill_method=None) * 100) for i in periods\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Moving Average"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:53.274435Z",
     "start_time": "2025-03-16T18:21:53.271380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sma(series: pd.Series, periods: list[int] | range, name: str, freq: str, compare=False, relative=False,\n",
    "        run_rate=False) -> dict[\n",
    "    str, pd.Series]:\n",
    "    def to_key(i: int):\n",
    "        return f\"{name}_sma_{i}{freq}\"\n",
    "\n",
    "    def to_compare_key(i: int):\n",
    "        return f\"{name}_vs_{name}_sma_{i}{freq}\"\n",
    "\n",
    "    def to_relative_key(i: int):\n",
    "        return f\"relative_{name}_{i}{freq}\"\n",
    "\n",
    "    def to_run_rate(i: int):\n",
    "        return f\"run_rate_{name}_{i}{freq}\"\n",
    "\n",
    "    cols = {\n",
    "        to_key(i): series.rolling(i).mean() for i in periods\n",
    "    }\n",
    "\n",
    "    if compare:\n",
    "        cols = cols | {\n",
    "            to_compare_key(i): (series - cols[to_key(i)]) / cols[to_key(i)] * 100 for i in periods\n",
    "        }\n",
    "\n",
    "    if relative:\n",
    "        cols = cols | {\n",
    "            to_relative_key(i): series / cols[to_key(i)] for i in periods\n",
    "        }\n",
    "\n",
    "    if run_rate:\n",
    "        cols = cols | {\n",
    "            to_run_rate(i): series / cols[to_key(i)] * 100 for i in periods\n",
    "        }\n",
    "\n",
    "    return cols\n",
    "\n",
    "def ema(series: pd.Series, periods: list[int] | range, name: str, freq: str, compare=False, relative=False,\n",
    "        run_rate=False) -> dict[\n",
    "    str, pd.Series]:\n",
    "    def to_key(i: int):\n",
    "        return f\"{name}_ema_{i}{freq}\"\n",
    "\n",
    "    def to_compare_key(i: int):\n",
    "        return f\"{name}_vs_{name}_ema_{i}{freq}\"\n",
    "\n",
    "    def to_relative_key(i: int):\n",
    "        return f\"relative_{name}_{i}{freq}\"\n",
    "\n",
    "    def to_run_rate(i: int):\n",
    "        return f\"run_rate_{name}_{i}{freq}\"\n",
    "\n",
    "    cols = {\n",
    "        to_key(i): series.rolling(i).mean() for i in periods\n",
    "    }\n",
    "\n",
    "    if compare:\n",
    "        cols = cols | {\n",
    "            to_compare_key(i): (series - cols[to_key(i)]) / cols[to_key(i)] * 100 for i in periods\n",
    "        }\n",
    "\n",
    "    if relative:\n",
    "        cols = cols | {\n",
    "            to_relative_key(i): series / cols[to_key(i)] for i in periods\n",
    "        }\n",
    "\n",
    "    if run_rate:\n",
    "        cols = cols | {\n",
    "            to_run_rate(i): series / cols[to_key(i)] * 100 for i in periods\n",
    "        }\n",
    "\n",
    "    return cols\n"
   ],
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Up Down"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:54.033855Z",
     "start_time": "2025-03-16T18:21:54.031233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def up_down(candle: pd.DataFrame, freq: str, period: list[int]) -> dict[str, pd.Series]:\n",
    "    cols: dict[str, pd.Series] = {}\n",
    "    for i in period:\n",
    "        c = candle.tail(i)\n",
    "        change = c.close - c.open\n",
    "        up = c[change > 0].volume.sum()\n",
    "        down = c[change < 0].volume.sum()\n",
    "        cols[f\"up_down_day_{i}{freq}\"] = up / down if down != 0 else np.nan\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Gap"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:54.527004Z",
     "start_time": "2025-03-16T18:21:54.524937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gap(candle: pd.DataFrame, freq: str):\n",
    "    prev_close = candle.close.shift(1)\n",
    "    gap_dollar = candle.open - prev_close\n",
    "    gap_pct = gap_dollar / prev_close * 100\n",
    "    unfilled_gap_dollar = ((candle.low.where(candle.low > prev_close, other=np.nan) - prev_close)\n",
    "                           .where(candle.high < prev_close, candle.high - prev_close))\n",
    "\n",
    "    unfilled_gap_pct = unfilled_gap_dollar / prev_close * 100\n",
    "    return {\n",
    "        f\"gap_dollar_{freq}\": gap_dollar,\n",
    "        f\"unfilled_gap_{freq}\": unfilled_gap_dollar,\n",
    "        f\"gap_pct_{freq}\": gap_pct,\n",
    "        f\"unfilled_gap_pct_{freq}\": unfilled_gap_pct\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Price Compare"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:55.040706Z",
     "start_time": "2025-03-16T18:21:55.038451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def price_compare(d: pd.DataFrame):\n",
    "    prev = d.shift(1)\n",
    "    prev_high = prev.high\n",
    "    prev_low = prev.low\n",
    "    prev_close = prev.close\n",
    "    prev_open = prev.open\n",
    "    return {\n",
    "        \"day_high_gt_prev_high\": d.high > prev_high,\n",
    "        \"day_low_gt_prev_low\": d.low > prev_low,\n",
    "        \"day_open_gt_prev_open\": d.open > prev_open,\n",
    "        \"day_close_gt_prev_close\": d.close > prev_close,\n",
    "        \"day_high_lt_prev_high\": d.high < prev_high,\n",
    "        \"day_low_lt_prev_low\": d.low < prev_low,\n",
    "        \"day_open_lt_prev_open\": d.open < prev_open,\n",
    "        \"day_close_lt_prev_close\": d.close < prev_close,\n",
    "        \"day_open_eq_high\": d.open == d.high,\n",
    "        \"day_open_eq_low\": d.open == d.low,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:55.679186Z",
     "start_time": "2025-03-16T18:21:55.676919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def price_performance(d: pd.DataFrame):\n",
    "    return {\n",
    "        \"price_perf_1D\": d.close.pct_change(1) * 100,\n",
    "        \"price_perf_1W\": d.close.pct_change(5) * 100,\n",
    "        \"price_perf_1M\": d.close.pct_change(21) * 100,\n",
    "        \"price_perf_3M\": d.close.pct_change(3 * 21) * 100,\n",
    "        \"price_perf_6M\": d.close.pct_change(6 * 21) * 100,\n",
    "        \"price_perf_9M\": d.close.pct_change(9 * 21) * 100,\n",
    "        \"price_perf_12M\": d.close.pct_change(12 * 21) * 100,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Price Action All"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:56.429397Z",
     "start_time": "2025-03-16T18:21:56.424387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def price_action(\n",
    "        d: pd.DataFrame,\n",
    "        d_w52: pd.DataFrame,\n",
    "        w: pd.DataFrame,\n",
    "        m: pd.DataFrame,\n",
    "        y: pd.DataFrame,\n",
    "        days_since_earning: int,\n",
    "        last_trading_day: pd.Timestamp,\n",
    ") -> dict[str, pd.Series]:\n",
    "    high_52_week = d_w52.high.max()\n",
    "    low_52_week = d_w52.low.min()\n",
    "    all_time_high = d.high.max()\n",
    "    all_time_low = d.low.min()\n",
    "    earning_open = d.open.shift(days_since_earning)\n",
    "\n",
    "    #VWAP\n",
    "    cols = vwap(d, 'daily') | vwap(w, 'weekly') | vwap(m, 'monthly') | vwap(y, 'yearly')\n",
    "\n",
    "    # Price Change\n",
    "    cols = cols | price_change_close(d, [1, 2, 3, 4], 'D') | price_change_close(w, range(1, 4), 'W')\n",
    "    cols = cols | price_change_close(m, range(1, 12), 'M') | price_change_close(y, range(1, 5), 'Y')\n",
    "\n",
    "    # Price Performance\n",
    "    cols = cols | price_performance(d)\n",
    "\n",
    "    # SMA\n",
    "    cols = cols | sma(d.close, [5, 10, 20, 30, 40, 50, 100, 200], 'price', 'D', compare=True)\n",
    "    cols = cols | sma(w.close, [10, 20, 30, 40, 50], 'price', 'W', compare=True)\n",
    "\n",
    "    # EMA\n",
    "    cols = cols | ema(d.close, [5, 10, 21, 30, 40, 50, 65], 'price', 'D', compare=True)\n",
    "\n",
    "    #High Low\n",
    "    cols = cols | {\n",
    "        \"price_change_today_pct\": cols['price_change_pct_1D'],\n",
    "        \"price_change_prev_week_close_pct\": cols['price_change_pct_1W'],\n",
    "        \"high_52_week\": high_52_week,\n",
    "        \"low_52_week\": low_52_week,\n",
    "        \"high_52_week_today\": d_w52.high.idxmax() == last_trading_day,\n",
    "        \"low_52_week_today\": d_w52.low.idxmax() == last_trading_day,\n",
    "        \"away_from_52_week_high_pct\": (d_w52.close - high_52_week) / high_52_week * 100,\n",
    "        \"away_from_52_week_low_pct\": (d_w52.close - low_52_week) / low_52_week * 100,\n",
    "        \"all_time_high\": all_time_high,\n",
    "        \"all_time_low\": all_time_low,\n",
    "        \"all_time_high_today\": d.high.idxmax() == last_trading_day,\n",
    "        \"all_time_low_today\": d.low.idxmax() == last_trading_day,\n",
    "        \"away_from_all_time_high_pct\": (d.close - all_time_high) / all_time_high * 100,\n",
    "        \"away_from_all_time_low_pct\": (d.close - all_time_low) / all_time_low * 100,\n",
    "    }\n",
    "\n",
    "    # Recent Price Change Comparison abs\n",
    "    cols = cols | {\n",
    "        \"price_change_today_abs\": d.close - d.close.shift(1),\n",
    "        \"price_change_from_open_abs\": d.close - d.open,\n",
    "        \"price_change_from_high_abs\": d.close - d.high,\n",
    "        \"price_change_from_low_abs\": d.close - d.low,\n",
    "    }\n",
    "\n",
    "    # Recent Price Change Comparison PCT\n",
    "    cols = cols | {\n",
    "        \"price_change_from_open_pct\": cols['price_change_from_open_abs'] / d.open * 100,\n",
    "        \"price_change_from_high_pct\": cols['price_change_from_high_abs'] / d.high * 100,\n",
    "        \"price_change_from_low_pct\": cols['price_change_from_low_abs'] / d.low * 100,\n",
    "        \"price_change_curr_week_open_pct\": (w.close - w.open) / w.open * 100,\n",
    "        \"price_change_since_earning_pct\": (d.close - earning_open) / earning_open * 100,\n",
    "    }\n",
    "\n",
    "    # Closing Range\n",
    "    cols = cols | {\n",
    "        \"dcr\": ((d.close - d.low) / (d.high - d.low)) * 100,\n",
    "        \"wcr\": ((w.close - w.low) / (w.high - w.low)) * 100,\n",
    "        \"mcr\": ((m.close - m.low) / (m.high - m.low)) * 100,\n",
    "    }\n",
    "\n",
    "    # Gaps\n",
    "    cols = cols | gap(d, \"D\") | gap(w, \"W\") | gap(m, \"M\")\n",
    "\n",
    "    # Up/Down\n",
    "    cols = cols | up_down(d, \"D\", [20, 50])\n",
    "\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Volume Action"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:57.777984Z",
     "start_time": "2025-03-16T18:21:57.774907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def volume_action(\n",
    "        d: pd.DataFrame,\n",
    "        d_w52: pd.DataFrame,\n",
    "        d_since_earning: pd.DataFrame,\n",
    "        w: pd.DataFrame,\n",
    "        row,\n",
    "        last_trading_day\n",
    "):\n",
    "    cols = {\n",
    "        \"highest_vol_since_earning\": False if len(\n",
    "            d_since_earning.volume) == 0 else d_since_earning.volume.idxmax() == last_trading_day,\n",
    "        \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
    "        \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
    "        \"vol_vs_yesterday_vol\": d.volume.pct_change(periods=1, fill_method=None) * 100,\n",
    "        \"week_vol_vs_prev_week_vol\": w.volume.pct_change(periods=1, fill_method=None) * 100,\n",
    "    }\n",
    "\n",
    "    # SMA\n",
    "    daily_periods = [5, 10, 20, 30, 40, 50, 100, 200]\n",
    "    weekly_periods = [10, 20, 30, 40, 50]\n",
    "    cols = cols | sma(d.volume, daily_periods, 'vol', 'D', compare=True, relative=True, run_rate=True)\n",
    "    cols = cols | sma(w.volume, weekly_periods, 'vol', 'W', compare=True, relative=True, run_rate=True)\n",
    "\n",
    "    # Price Volume\n",
    "    price_volume = d.close * d.volume\n",
    "    cols = cols | {\"price_volume\": price_volume}\n",
    "    cols = cols | sma(price_volume, daily_periods, 'price_volume', 'D')\n",
    "\n",
    "    # Float Turnover\n",
    "    total_float = row.shares_float\n",
    "    float_turnover = d.volume / total_float * 100\n",
    "    cols = cols | {\"float_turnover\": float_turnover}\n",
    "    cols = cols | sma(float_turnover, daily_periods, 'float_turnover', 'D', compare=True)\n",
    "\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Custom Pattern and Indicators"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pocket Pivot"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:21:59.621283Z",
     "start_time": "2025-03-16T18:21:59.618741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pocket_pivot(d: pd.DataFrame, prev: pd.DataFrame):\n",
    "    price_check = d.close > prev.close\n",
    "    negative_volume = d.volume.where(d.close < prev.close, 0)\n",
    "    max_negative_vol_in10_days = negative_volume.rolling(window=10, min_periods=1).max()\n",
    "    volume_check = d.volume > max_negative_vol_in10_days\n",
    "    return price_check & volume_check\n"
   ],
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Minicoil"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:00.156494Z",
     "start_time": "2025-03-16T18:22:00.154176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def minicoil(d: pd.DataFrame, prev: pd.DataFrame, prev_2: pd.DataFrame):\n",
    "    return (\n",
    "            (prev.close < prev_2.close) & (prev.low > prev_2.low)  # day2_inside\n",
    "            &\n",
    "            (d.high < prev_2.high) & (d.low > prev_2.low)  # day1_inside\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3 Weeks Tight"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:00.683742Z",
     "start_time": "2025-03-16T18:22:00.681821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def three_week_tight(w: pd.DataFrame):\n",
    "    rolling_3 = w.close.rolling(window=3)\n",
    "    # Range is within 1.5%\n",
    "    return ((rolling_3.max() - rolling_3.min()) / rolling_3.mean()) <= 0.015"
   ],
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5 Weeks Up"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:01.141008Z",
     "start_time": "2025-03-16T18:22:01.139084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def five_week_up(w: pd.DataFrame):\n",
    "    w_close = w.close\n",
    "    w_close1 = w_close.shift(1)\n",
    "    w_close2 = w_close.shift(2)\n",
    "    w_close3 = w_close.shift(3)\n",
    "    w_close4 = w.close.shift(4)\n",
    "    w_close5 = w.close.shift(5)\n",
    "\n",
    "    return ((w_close > w_close1) &\n",
    "            (w_close1 > w_close2) &\n",
    "            (w_close2 > w_close3) &\n",
    "            (w_close3 > w_close4) &\n",
    "            (w_close4 > w_close5))"
   ],
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### High Tight Flag"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:01.695774Z",
     "start_time": "2025-03-16T18:22:01.693831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def high_tight_flag(d: pd.DataFrame):\n",
    "    rolling8 = d.close.rolling(window=8)\n",
    "    rolling3_high = d.high.rolling(window=3)\n",
    "    rolling3_low = d.low.rolling(window=3)\n",
    "    rolling_3_close = d.close.rolling(window=3)\n",
    "    # 90% sharp move\n",
    "    sharp_move = rolling8.max() / rolling8.min() >= 1.90\n",
    "    # Tight (<= 25%) consolidation\n",
    "    consolidation = (rolling3_high.max() - rolling3_low.min()) / rolling_3_close.mean() <= 0.025\n",
    "    return sharp_move & consolidation"
   ],
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Ants"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:02.243162Z",
     "start_time": "2025-03-16T18:22:02.240997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ants(d: pd.DataFrame, prev: pd.DataFrame):\n",
    "    return (\n",
    "            ((d.close > prev.close).rolling(window=15).sum() >= 12)  # 12/15 days up\n",
    "            &\n",
    "            (d.volume > d.volume.rolling(window=15).mean())  # Increase in average volume\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Power Trend"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:02.752041Z",
     "start_time": "2025-03-16T18:22:02.750174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def power_trend(d: pd.DataFrame, ema_21: pd.Series, sma_50: pd.Series):\n",
    "    return (\n",
    "            (d.close > ema_21)  # Close above 21\n",
    "            &\n",
    "            (ema_21 > sma_50)  # 21 EMA > 50 SMA\n",
    "            &\n",
    "            (d.close > sma_50)  # Close above 50 SMA\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Power of Three"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:03.255945Z",
     "start_time": "2025-03-16T18:22:03.254033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def power_of_three(d: pd.DataFrame, ema_10: pd.Series, ema_21: pd.Series, sma_50: pd.Series):\n",
    "    return (\n",
    "            ta.cross(d.close, ema_10)  # Close above 10\n",
    "            &\n",
    "            ta.cross(d.close, ema_21)  # Close above 21\n",
    "            &\n",
    "            ta.cross(d.close, sma_50)  # Close above 50 SMA\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Launchpad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:03.780817Z",
     "start_time": "2025-03-16T18:22:03.778909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def launchpad_daily(ema_21: pd.Series, sma_50: pd.Series):\n",
    "    # Short and long-term MAs close to each other (< 2%)\n",
    "    return (ema_21 / sma_50 - 1).abs() < 0.02\n",
    "\n",
    "\n",
    "def launchpad_weekly(ema_5: pd.Series, sma_10: pd.Series):\n",
    "    # Short and long-term MAs close to each other (< 2%)\n",
    "    return (ema_5 / sma_10 - 1).abs() < 0.02"
   ],
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Sigma Spike"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:04.300876Z",
     "start_time": "2025-03-16T18:22:04.299195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigma_spike(d: pd.DataFrame):\n",
    "    # Calculate daily percent change\n",
    "    day_chang_pct = ta.percent_return(d.close) * 100\n",
    "    # Calculate standard deviation of daily percent changes over the past 20 days\n",
    "    volatility_20_days = day_chang_pct.rolling(window=20).std()\n",
    "    # Calculate Sigma Spike\n",
    "    return day_chang_pct / volatility_20_days"
   ],
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Historical SMA Comparision"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:04.894996Z",
     "start_time": "2025-03-16T18:22:04.892893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sma_comparison(d: pd.DataFrame, sma_200: pd.Series):\n",
    "    return {\n",
    "        f\"sma_200_vs_sma_200_{i}M_ago\": sma_200 > ta.sma(d.close.shift(21 * i)) for i in range(7)\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Stage Analysis"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:06.297507Z",
     "start_time": "2025-03-16T18:22:06.295694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stan_weinstein_stage_analysis(d: pd.DataFrame):\n",
    "    # TODO\n",
    "    # Stan\n",
    "    # Weinstein\n",
    "    # Stages(1\n",
    "    # A, 1, 2\n",
    "    # A, 2, 3\n",
    "    # A, 3, 4, 4\n",
    "    # B -)  #\n",
    "    return \"-\""
   ],
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### SMA EMA Slope"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:06.683287Z",
     "start_time": "2025-03-16T18:22:06.681109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sma_vs_ema_slope(d: pd.DataFrame, freq: str, period: list[int]):\n",
    "    cols = {}\n",
    "    for i in period:\n",
    "        ma = d.close.rolling(i).mean()\n",
    "        ma_shifted = ma.shift(1)\n",
    "        ma_slope = (ma - ma_shifted) / ma_shifted * 100\n",
    "        adr_10 = (d.high - d.low).rolling(10).mean() / d.close * 100\n",
    "        ma_vs_adr_10 = ma_slope / adr_10\n",
    "\n",
    "        cols[f\"sma_{i}{freq}\"] = ma\n",
    "        cols[f\"{i}{freq}sma_vs_ema_slope_pct\"] = ma_slope\n",
    "        cols[f\"{i}{freq}sma_vs_ema_slope_adr\"] = ma_vs_adr_10\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Relative Strength"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:08.008719Z",
     "start_time": "2025-03-16T18:22:08.004042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relative_strength(d: pd.DataFrame, market_close: pd.Series):\n",
    "    common_dates = d.index.intersection(market_close.index)\n",
    "    d = d.loc[common_dates]\n",
    "    market_close = market_close.loc[common_dates]\n",
    "\n",
    "    rs_day_periods = [5, 10, 15, 20, 25, 30, 60, 90]\n",
    "\n",
    "    symbol_return = ta.percent_return(d.close)\n",
    "    market_return = ta.percent_return(market_close)\n",
    "    rs_day = symbol_return > market_return\n",
    "\n",
    "    # Relative Strength\n",
    "    cols = {}\n",
    "    for i in rs_day_periods:\n",
    "        days = rs_day.rolling(i).sum()\n",
    "        cols[f\"RS_{i}D\"] = days\n",
    "        cols[f\"RS_{i}D_pct\"] = days / i * 100\n",
    "\n",
    "    # Relative Strength New High\n",
    "    rsnh_period_month = [1, 3, 6, 9, 12]\n",
    "    for i in rsnh_period_month:\n",
    "        # Relative Strength New High\n",
    "        widow = i * 21\n",
    "\n",
    "        sym_return = d.close / d.close.shift(widow)\n",
    "        mrk_return = market_close / market_close.shift(widow)\n",
    "\n",
    "        rs_line: pd.Series = sym_return / mrk_return\n",
    "        latest_rs_line = 0 if rs_line.empty else rs_line.iloc[-1]\n",
    "\n",
    "        rsnh = latest_rs_line == rs_line.rolling(widow).max()\n",
    "        cols[f\"RSNH_{i}M\"] = rsnh\n",
    "\n",
    "        # Relative Strength New High Before Price\n",
    "        stock_high = d.close == d.close.rolling(widow).max()\n",
    "        rsnhbp = rsnh & ~stock_high\n",
    "        cols[f\"RSNHBP_{i}M\"] = rsnhbp\n",
    "\n",
    "    dpm = 21\n",
    "    dpw = 5\n",
    "    cols[\"RS_Value_1D\"] = ((d.close / d.close.shift(1)) / (market_close / market_close.shift(1))) - 1\n",
    "    cols[\"RS_Value_1W\"] = ((d.close / d.close.shift(dpw)) / (market_close / market_close.shift(dpw))) - 1\n",
    "    cols[\"RS_Value_1M\"] = ((d.close / d.close.shift(dpm)) / (market_close / market_close.shift(dpm))) - 1\n",
    "    cols[\"RS_Value_3M\"] = ((d.close / d.close.shift(3 * dpm)) / (market_close / market_close.shift(3 * dpm))) - 1\n",
    "    cols[\"RS_Value_6M\"] = ((d.close / d.close.shift(6 * dpm)) / (market_close / market_close.shift(6 * dpm))) - 1\n",
    "    cols[\"RS_Value_9M\"] = ((d.close / d.close.shift(9 * dpm)) / (market_close / market_close.shift(9 * dpm))) - 1\n",
    "    cols[\"RS_Value_12M\"] = ((d.close / d.close.shift(12 * dpm)) / (market_close / market_close.shift(12 * dpm))) - 1\n",
    "\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ADR"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:12.544236Z",
     "start_time": "2025-03-16T18:22:12.541901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def adr(d: pd.DataFrame):\n",
    "    period = [1, 2, 5, 10, 14, 20]\n",
    "    cols = {}\n",
    "    for i in period:\n",
    "        rng = d.high - d.low\n",
    "        adr_value = rng.rolling(i).mean()\n",
    "        adr_pct = adr_value / d.close * 100\n",
    "        cols[f\"ADR_{i}D\"] = adr_value\n",
    "        cols[f\"ADR_pct_{i}D\"] = adr_pct\n",
    "\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ATR"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:13.278939Z",
     "start_time": "2025-03-16T18:22:13.276532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def atr(d: pd.DataFrame):\n",
    "    period = [2, 5, 10, 14, 20]\n",
    "    cols = {}\n",
    "    for i in period:\n",
    "        cols[f\"ATR_{i}D\"] = ta.atr(d.high, d.low, d.close, i)\n",
    "\n",
    "    return cols\n"
   ],
   "outputs": [],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Alpha"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:14.157560Z",
     "start_time": "2025-03-16T18:22:14.155300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def alpha(d: pd.DataFrame, market_close: pd.Series):\n",
    "    #6 Month\n",
    "    market_return = ta.percent_return(market_close, 6 * 21)\n",
    "    symbol_return = ta.percent_return(d.close, 6 * 21)\n",
    "    cols = {\n",
    "        \"alpha_6M\": np.nan if market_return is None or symbol_return is None else (symbol_return - market_return) * 100,\n",
    "    }\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Candlestick Pattern Utils"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:15.594238Z",
     "start_time": "2025-03-16T18:22:15.591872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def safe_call_cdl_pattern(d: pd.DataFrame, name: str, bearish=False) -> pd.Series | bool:\n",
    "    # noinspection PyBroadException\n",
    "    try:\n",
    "        result = ta.cdl(d.open, d.high, d.low, d.close, name=name)\n",
    "        if len(result.columns) == 0:\n",
    "            result = None\n",
    "    except:\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        return False\n",
    "\n",
    "    series = result[result.columns[0]]\n",
    "    if bearish:\n",
    "        return series < 0\n",
    "    return series > 0"
   ],
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Indicators Main"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:17.063008Z",
     "start_time": "2025-03-16T18:22:17.057484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def indicators(d: pd.DataFrame, w: pd.DataFrame, market_close: pd.Series):\n",
    "    # Link: https://deepvue.com/knowledge-base/technical/\n",
    "    prev = d.shift(1)\n",
    "    prev_2 = d.shift(2)\n",
    "    prev_w = w.shift(1)\n",
    "    ema_4_high = d.high.ewm(span=4).mean()\n",
    "    ema_10_close = d.close.ewm(span=10).mean()\n",
    "    ema_21_close = d.close.ewm(span=21).mean()\n",
    "    sma_50_close = d.close.rolling(50).mean()\n",
    "    w_ema_5_close = w.close.ewm(span=5).mean()\n",
    "    w_sma_10_close = w.close.rolling(10).mean()\n",
    "\n",
    "    inside = safe_call_cdl_pattern(d, name='inside')\n",
    "    inside_week = safe_call_cdl_pattern(w, name='inside')\n",
    "    cols = {\n",
    "        \"inside_day\": inside,\n",
    "        \"double_inside_day\": inside & safe_call_cdl_pattern(prev, name='inside'),\n",
    "        \"inside_week\": inside_week,\n",
    "        \"double_inside_week\": inside_week & safe_call_cdl_pattern(prev_w, name='inside'),\n",
    "        \"outside_day\": safe_call_cdl_pattern(d, name='engulfing'),\n",
    "        \"outside_week\": safe_call_cdl_pattern(w, name='engulfing'),\n",
    "        \"outside_bullish_day\": (d.open < prev.low) & (d.close > prev.high),\n",
    "        \"outside_bearish_day\": (d.open > prev.high) & (d.close < prev.low),\n",
    "        \"outside_bullish_week\": (w.open < prev_w.low) & (w.close > prev_w.high),\n",
    "        \"outside_bearish_week\": (w.open > prev_w.high) & (w.close < prev_w.low),\n",
    "        \"wick_play\": ((d.low > prev.open) | (d.low > prev.close)) & (d.high < prev.high),\n",
    "        \"in_the_wick\": (d.open < prev.high) & ((d.low > prev.low) | (d.open > prev.high)),\n",
    "        \"3_line_strike_bullish\": safe_call_cdl_pattern(d, name='3linestrike'),\n",
    "        \"3_line_strike_bearish\": safe_call_cdl_pattern(d, name='3linestrike', bearish=True),\n",
    "        \"3_bar_break\": d.close > prev.high.rolling(3).max(),\n",
    "        \"bullish_reversal\": (d.low < prev.low) & d.close > prev.close,\n",
    "        \"upside_reversal\": (d.low < prev.low) & (d.close > (d.high + d.low) / 2),\n",
    "        \"oops_reversal\": (d.open < prev.low) & (d.close > prev.low),\n",
    "        \"key_reversal\": (d.open < prev.low) & (d.close < prev.high),\n",
    "        \"pocket_pivot\": pocket_pivot(d, prev),\n",
    "        \"volume_dry_up\": d.volume == d.volume.rolling(window=10, min_periods=1).min(),\n",
    "        \"slingshot\": (d.close > ema_4_high) & (d.close <= ema_4_high.shift(1)),\n",
    "        \"minicoil\": minicoil(d, prev, prev_2),\n",
    "        \"3_week_tight\": three_week_tight(w),\n",
    "        \"5_week_up\": five_week_up(w),\n",
    "        \"high_tight_flag\": high_tight_flag(d),\n",
    "        \"ants\": ants(d, prev),\n",
    "        \"power_trend\": power_trend(d, ema_21_close, sma_50_close),\n",
    "        \"power_of_three\": power_of_three(d, ema_10_close, ema_21_close, sma_50_close),\n",
    "        \"launchpad\": launchpad_daily(ema_21_close, sma_50_close),\n",
    "        \"launchpad_weekly\": launchpad_weekly(w_ema_5_close, w_sma_10_close),\n",
    "        #TODO: Green Line Breakout\n",
    "        \"doji\": safe_call_cdl_pattern(d, name='doji'),\n",
    "        \"morning_star\": safe_call_cdl_pattern(d, name='morningstar'),\n",
    "        \"evening_star\": safe_call_cdl_pattern(d, name='eveningstar'),\n",
    "        \"shooting_star\": safe_call_cdl_pattern(d, name='shootingstar'),\n",
    "        \"hammer\": safe_call_cdl_pattern(d, name='hammer'),\n",
    "        \"inverted_hammer\": safe_call_cdl_pattern(d, name='invertedhammer'),\n",
    "        \"bullish_harami\": safe_call_cdl_pattern(d, name='harami'),\n",
    "        \"bearish_harami\": safe_call_cdl_pattern(d, name='harami', bearish=False),\n",
    "        #TODO: Bullish engulfing and bearish engulfing\n",
    "        #TODO: Bullish kicker and bearish engulfing,\n",
    "        \"piercing_line\": safe_call_cdl_pattern(d, name='piercing'),\n",
    "        \"hanging_man\": safe_call_cdl_pattern(d, name='hangingman'),\n",
    "        \"dark_cloud_cover\": safe_call_cdl_pattern(d, name='darkcloudcover'),\n",
    "        \"gravestone_doji\": safe_call_cdl_pattern(d, name='gravestonedoji'),\n",
    "        \"3_back_crows\": safe_call_cdl_pattern(d, name='3blackcrows'),\n",
    "        \"dragonfly_doji\": safe_call_cdl_pattern(d, name='dragonflydoji'),\n",
    "        \"3_white_soldiers\": safe_call_cdl_pattern(d, name='3whitesoldiers'),\n",
    "        \"sigma_spike\": sigma_spike(d),\n",
    "        \"stan_weinstein_stage\": stan_weinstein_stage_analysis(d),\n",
    "    }\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Technicals"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:18.859536Z",
     "start_time": "2025-03-16T18:22:18.856731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def technical(d: pd.DataFrame, w: pd.DataFrame, market_close: pd.Series, row):\n",
    "    cols = price_compare(d)\n",
    "    sma_200_close = d.close.rolling(200).mean()\n",
    "    try:\n",
    "        cols = cols | indicators(d, w, market_close)\n",
    "        #Alpha\n",
    "        cols = cols | alpha(d, market_close)\n",
    "        # SMA Comparison Months Back\n",
    "        cols = cols | sma_comparison(d, sma_200_close)\n",
    "        # SMA Comparison with EMA Slop\n",
    "        cols = cols | sma_vs_ema_slope(d, \"D\", [10, 20, 30, 40, 50, 100, 200])\n",
    "        # Relative Strength\n",
    "        cols = cols | relative_strength(d, market_close)\n",
    "        # ADR\n",
    "        cols = cols | adr(d)\n",
    "        # ATR\n",
    "        cols = cols | atr(d)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in technical indicators: \", row.name)\n",
    "        print(d.head())\n",
    "        print(market_close.head())\n",
    "        raise e\n",
    "    return cols"
   ],
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Technical Process for Symbol"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:22.844071Z",
     "start_time": "2025-03-16T18:22:22.840275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_candles(row):\n",
    "    ticker = row.name\n",
    "    d = daily_candles[ticker]\n",
    "    w = weekly_candles[ticker]\n",
    "    m = monthly_candles[ticker]\n",
    "    y = yearly_candle[ticker]\n",
    "    market_close = daily_candles[market_ticker[row.exchange]].reindex(d.index).close.fillna(0)\n",
    "    if d.volume.empty:\n",
    "        d['volume'] = np.nan\n",
    "\n",
    "    last_trading_day = d.index[-1]\n",
    "    year_back = last_trading_day - pd.DateOffset(years=1)\n",
    "    d_w52 = d[d.index > year_back]\n",
    "\n",
    "    last_earning_date = row.earnings_release_trading_date\n",
    "    d_since_earning = d[d.index >= last_earning_date]\n",
    "\n",
    "    #Meta\n",
    "    days_since_earning = len(d_since_earning)\n",
    "    cols = {\"days_since_latest_earning\": pd.Series([days_since_earning])}\n",
    "\n",
    "    #OHLCV\n",
    "    cols = cols | ohlcv(d, name='day') | ohlcv(d, name='prev_day', shift=1)\n",
    "    cols = cols | ohlcv(w, name='week') | ohlcv(w, name='prev_week', shift=1)\n",
    "    cols = cols | ohlcv(m, name='month') | ohlcv(m, name='prev_month', shift=1)\n",
    "    cols = cols | ohlcv(y, name='year') | ohlcv(y, name='prev_year', shift=1)\n",
    "\n",
    "    # Price Action\n",
    "    cols = cols | price_action(d, d_w52, w, m, y, days_since_earning, last_trading_day)\n",
    "\n",
    "    # Volume Action\n",
    "    cols = cols | volume_action(d, d_w52, d_since_earning, w, row, last_trading_day)\n",
    "\n",
    "    # Technical\n",
    "    cols = cols | technical(d, w, market_close, row)\n",
    "\n",
    "    return {k: get_latest(v) for k, v in cols.items()}\n"
   ],
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Technincal Main"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:22:57.152439Z",
     "start_time": "2025-03-16T18:22:24.572830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "######################## ~~~Technical~~~~ ########################\n",
    "df['no_volume'] = q['has-no-volume']\n",
    "# Beta\n",
    "df['beta_1_year'] = q['beta_1_year']\n",
    "df['beta_3_year'] = q['beta_3_year']\n",
    "df['beta_5_year'] = q['beta_5_year']\n",
    "\n",
    "# To Weekly, Monthly and Yearly Candle\n",
    "market_ticker = {\"NSE\": \"NSE:CNX500\", \"BSE\": \"NSE:CNX500\"}\n",
    "weekly_candles = {ticker: to_weekly_candles(d) for ticker, d in daily_candles.items()}\n",
    "monthly_candles = {ticker: to_monthly_candles(d) for ticker, d in daily_candles.items()}\n",
    "yearly_candle = {ticker: to_yearly_candles(d) for ticker, d in monthly_candles.items()}\n",
    "ta_metrics_data = df.apply(process_candles, axis=1)\n",
    "ta_df: pd.DataFrame = ta_metrics_data.apply(pd.Series)\n",
    "df = df.join(ta_df)\n",
    "######################## ~~~Technical~~~~ ########################"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:12: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
      "/var/folders/6j/dvdvpwvj2gq3fdjs2dw420c40000gn/T/ipykernel_67427/2344826140.py:13: FutureWarning: The behavior of Series.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n"
     ]
    }
   ],
   "execution_count": 182
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rating"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rating Utils"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:40:34.709196Z",
     "start_time": "2025-03-16T18:40:34.707330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_rating(series: pd.Series):\n",
    "    # Rank stocks, convert to percentile (0-1), scale to 1-99\n",
    "    percentile = series.rank(pct=True)  # Gives values between 0 and 1\n",
    "    asr = percentile * 98 + 1  # Scale to 1-99\n",
    "    return asr.fillna(1).round().astype(int)  # Round to integer"
   ],
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Absolute Strength Rating"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:40:35.786600Z",
     "start_time": "2025-03-16T18:40:35.781461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rank should  be between 1 to 99\n",
    "df['AS_Rating_1D'] = compute_rating(df['price_perf_1D'])\n",
    "df['AS_Rating_1W'] = compute_rating(df['price_perf_1W'])\n",
    "df['AS_Rating_1M'] = compute_rating(df['price_perf_1M'])\n",
    "df['AS_Rating_3M'] = compute_rating(df['price_perf_3M'])\n",
    "df['AS_Rating_6M'] = compute_rating(df['price_perf_6M'])\n",
    "df['AS_Rating_9M'] = compute_rating(df['price_perf_9M'])\n",
    "df['AS_Rating_12M'] = compute_rating(df['price_perf_12M'])"
   ],
   "outputs": [],
   "execution_count": 237
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Relative Strength Rating"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:40:37.755686Z",
     "start_time": "2025-03-16T18:40:37.750204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['RS_Rating_1D'] = compute_rating(df['RS_Value_1D'])\n",
    "df['RS_Rating_1W'] = compute_rating(df['RS_Value_1W'])\n",
    "df['RS_Rating_1M'] = compute_rating(df['RS_Value_1M'])\n",
    "df['RS_Rating_3M'] = compute_rating(df['RS_Value_3M'])\n",
    "df['RS_Rating_6M'] = compute_rating(df['RS_Value_6M'])\n",
    "df['RS_Rating_9M'] = compute_rating(df['RS_Value_9M'])\n",
    "df['RS_Rating_12M'] = compute_rating(df['RS_Value_12M'])"
   ],
   "outputs": [],
   "execution_count": 238
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sector/Industry/Sub Industry Rating"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:21:24.094340Z",
     "start_time": "2025-03-16T20:21:24.090114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def group_strength_rating(df: pd.DataFrame, group_col: str, method1_weight=0.7):\n",
    "    # Step 1: Filter stocks based on price_volume\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # Step 2: Column mapping\n",
    "    asr_cols_mapping = {\n",
    "        \"AS_Rating_1D\": f\"{group_col}_rating_1D\",\n",
    "        \"AS_Rating_1W\": f\"{group_col}_rating_1W\",\n",
    "        \"AS_Rating_1M\": f\"{group_col}_rating_1M\",\n",
    "        \"AS_Rating_3M\": f\"{group_col}_rating_3M\",\n",
    "        \"AS_Rating_6M\": f\"{group_col}_rating_6M\",\n",
    "        \"AS_Rating_9M\": f\"{group_col}_rating_9M\",\n",
    "        \"AS_Rating_12M\": f\"{group_col}_rating_12M\",\n",
    "    }\n",
    "    asr_cols = list(asr_cols_mapping.keys())\n",
    "    rank_cols = list(asr_cols_mapping.values())\n",
    "\n",
    "    # Step 3: Aggregate mean & median scores\n",
    "    mean_scores = df_filtered.groupby(group_col)[asr_cols].mean()\n",
    "    median_scores = df_filtered.groupby(group_col)[asr_cols].median()\n",
    "    combined_scores = method1_weight * mean_scores + (1 - method1_weight) * median_scores\n",
    "\n",
    "    # Step 4: Rank groups (1 = best, N = worst)\n",
    "    for col in asr_cols:\n",
    "        rank_col = asr_cols_mapping[col]\n",
    "        combined_scores[rank_col] = combined_scores[col].rank(ascending=False, method='dense').astype(int)\n",
    "\n",
    "    # Step 5: Handle missing groups\n",
    "    all_groups = df[group_col].unique()\n",
    "    combined_scores = combined_scores.reindex(all_groups)\n",
    "\n",
    "    for col in asr_cols:\n",
    "        rank_col = asr_cols_mapping[col]\n",
    "        max_rank = combined_scores[rank_col].max()\n",
    "        combined_scores[rank_col] = combined_scores[rank_col].fillna(max_rank + 1).astype(int)\n",
    "\n",
    "    # Step 6: Merge rank columns back to original DataFrame\n",
    "    return df.copy().merge(combined_scores[rank_cols], how='left', left_on=group_col, right_index=True)"
   ],
   "outputs": [],
   "execution_count": 280
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T20:21:25.145757Z",
     "start_time": "2025-03-16T20:21:25.102881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = group_strength_rating(df, 'sector')\n",
    "df = group_strength_rating(df, 'industry')\n",
    "df = group_strength_rating(df, 'sub_industry')"
   ],
   "outputs": [],
   "execution_count": 281
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(df)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WjtIHryPRWm4",
    "ExecuteTime": {
     "end_time": "2025-03-16T08:21:17.422415Z",
     "start_time": "2025-03-16T08:21:17.420496Z"
    }
   },
   "source": [
    "def make_df_ready_for_serialization(df: pd.DataFrame):\n",
    "    df: pd.DataFrame = df.copy()\n",
    "    for col in df.columns:\n",
    "        # Replace NaN with None for PostgreSQL compatibility\n",
    "        df[col] = df[col].where(pd.notnull(df[col]), None)\n",
    "\n",
    "    df = df.replace('nan', None)\n",
    "    df = df.replace([np.inf, -np.inf], None)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fWCjTtlE4hcF",
    "ExecuteTime": {
     "end_time": "2025-03-16T08:21:17.586223Z",
     "start_time": "2025-03-16T08:21:17.446423Z"
    }
   },
   "source": "df = make_df_ready_for_serialization(df)",
   "outputs": [],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MLNhWGKP4mFo",
    "ExecuteTime": {
     "end_time": "2025-03-16T08:21:31.503896Z",
     "start_time": "2025-03-16T08:21:29.580533Z"
    }
   },
   "source": "df.to_parquet(f'oci://{oci_bucket}/symbols-full.parquet', compression='zstd', storage_options=storage_options)",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
