{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup\n"
  },
  {
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Parameters\n",
    "connection_string = \"DEFAULT\"\n",
    "install_ta_lib_binary = False\n",
    "install_deps = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaTQWubyR6ZK"
   },
   "outputs": [],
   "source": [
    "# If we are using this in collab, then connection_string will by default be DEFAULT and when trigger manually or by paper mill, it will be set using\n",
    "# parameter injection\n",
    "if connection_string == \"DEFAULT\":\n",
    "    print(\"No connection string provided... Using Collab Userdata\")\n",
    "    from google.colab import userdata\n",
    "\n",
    "    connection_string = userdata.get('PG_CONNECTION')\n",
    "\n",
    "if install_ta_lib_binary:\n",
    "    print(\"Installing ta lib binary\")\n",
    "    !wget https://github.com/ta-lib/ta-lib/releases/download/v0.6.3/ta-lib_0.6.3_amd64.deb\n",
    "    !dpkg -i ta-lib_0.6.3_amd64.deb\n",
    "\n",
    "if install_deps:\n",
    "    print(\"Install dependencies...\")\n",
    "    !pip install TA-Lib pandas-ta pandas numpy==1.26.4 tables websockets sqlalchemy --quiet"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import asyncio\n",
    "import json\n",
    "from logging import log\n",
    "from random import choices\n",
    "from string import ascii_letters, digits\n",
    "from typing import Any\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import requests\n",
    "from websockets import ConnectionClosed\n",
    "from websockets import Origin\n",
    "from websockets.asyncio.client import connect, ClientConnection"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if the DB Connect is set\n",
    "from sqlalchemy import create_engine,text\n",
    "engine = create_engine(connection_string, echo=False)\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT 1\"))\n",
    "    print(\"Database connection is successful.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Download"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_MESSAGE_PREFIX = \"~m~\"\n",
    "\n",
    "\n",
    "def chunk_list(lst: list[str], chunk_size: int):\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "async def download(tickers: list[str]):\n",
    "    all_quotes = {}\n",
    "    all_bars = {}\n",
    "    async  for quotes, bars in fetch_bulk(tickers):\n",
    "        all_quotes.update(quotes)\n",
    "        all_bars.update(bars)\n",
    "\n",
    "    return all_quotes, all_bars\n",
    "\n",
    "\n",
    "def to_quote_df(quotes: dict[str, dict]):\n",
    "    print(\"Generating Quote DataFrames...\")\n",
    "    quote_df = pd.DataFrame(quotes.values())\n",
    "    quote_df['ticker'] = quote_df['pro_name']\n",
    "    quote_df = quote_df.set_index(['ticker'])\n",
    "    print(\"Generated Quote DataFrames...\")\n",
    "    return quote_df\n",
    "\n",
    "\n",
    "def to_bars_df(bars: dict[str, list[list]]):\n",
    "    required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "    def process_bar(bar):\n",
    "        b_df = pd.DataFrame(bar)\n",
    "        # Add column names dynamically (truncate to available data)\n",
    "        b_df.columns = required_columns[:b_df.shape[1]]\n",
    "        # Ensure all required columns are present\n",
    "        for col in required_columns:\n",
    "            if col not in b_df.columns:\n",
    "                b_df[col] = np.nan  # Fill missing columns with NaN\n",
    "\n",
    "        b_df['timestamp'] = pd.to_datetime(b_df['timestamp'], unit='s')\n",
    "        b_df['timestamp'] = b_df['timestamp'].dt.floor('D')\n",
    "        b_df.set_index(['timestamp'], inplace=True)\n",
    "        return b_df\n",
    "\n",
    "    print(\"Generating Bar DataFrames...\")\n",
    "    v = {k: process_bar(bar) for k, bar in bars.items()}\n",
    "    print(\"Generated Bar DataFrames...\")\n",
    "    return v\n",
    "\n",
    "\n",
    "async def fetch_bulk(tickers: list[str]):\n",
    "    main_chunk = chunk_list(tickers, 500)\n",
    "    failed_chunks = []\n",
    "    for idx, chunk in enumerate(main_chunk):\n",
    "        print(f\"Started: {idx + 1}/{len(main_chunk)}\")\n",
    "        sub_chunks = chunk_list(chunk, 100)\n",
    "        tasks = [asyncio.create_task(fetch_data(chunked_symbols)) for chunked_symbols in sub_chunks]\n",
    "        chunk_result = await asyncio.gather(*tasks)\n",
    "        for chunked_symbols, result in zip(sub_chunks, chunk_result):\n",
    "            if result is None:\n",
    "                print(\"Failed chunks: \", chunked_symbols)\n",
    "                failed_chunks = failed_chunks + chunked_symbols\n",
    "                continue\n",
    "\n",
    "            yield result\n",
    "\n",
    "        print(f\"Completed: {idx + 1}/{len(main_chunk)}\")\n",
    "\n",
    "\n",
    "async def fetch_data(ticker: list[str], mode: Literal[\"quote\", \"bar\", \"all\"] = \"all\"):\n",
    "    if len(ticker) == 0:\n",
    "        return {}, {}\n",
    "\n",
    "    data = {}\n",
    "    complete = False\n",
    "    last_message = None\n",
    "    async with (connect_to_server() as socket):\n",
    "        try:\n",
    "            await _init(socket, ticker, data, mode)\n",
    "            async  for message in socket:\n",
    "                last_message = message\n",
    "                complete = await _process_data(socket, ticker, message, data)\n",
    "                if complete:\n",
    "                    break\n",
    "            await _end(socket)\n",
    "        except ConnectionClosed as e:\n",
    "            if not complete:\n",
    "                print(\"Failed\", last_message)\n",
    "                log.error(\"Connection Closed\", e)\n",
    "\n",
    "    if complete:\n",
    "        return data.get(\"quotes\", {}), data.get(\"bars\", {})\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def connect_to_server():\n",
    "    url = \"wss://data-wdc.tradingview.com/socket.io/websocket?type=chart\"\n",
    "    origin = \"https://in.tradingview.com\"\n",
    "    return connect(url, origin=Origin(origin), max_size=None, ping_timeout=60)\n",
    "\n",
    "\n",
    "def _encode(data: dict[str, Any] | list[dict[str, Any]] | str) -> str:\n",
    "    encoded_message = \"\"\n",
    "    if isinstance(data, str):\n",
    "        encoded_message += f\"{_MESSAGE_PREFIX}{len(data)}{_MESSAGE_PREFIX}{data}\"\n",
    "        return encoded_message\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "\n",
    "    for item in data:\n",
    "        stringified = json.dumps(item) if item is not None else \"\"\n",
    "        encoded_message += f\"{_MESSAGE_PREFIX}{len(stringified)}{_MESSAGE_PREFIX}{stringified}\"\n",
    "\n",
    "    return encoded_message\n",
    "\n",
    "\n",
    "async def _decode(socket: ClientConnection, msg: str) -> list[dict[str, Any]]:\n",
    "    decoded_messages = []\n",
    "    while msg.startswith(_MESSAGE_PREFIX):\n",
    "        msg = msg[len(_MESSAGE_PREFIX):]\n",
    "        separator_index = msg.find(_MESSAGE_PREFIX)\n",
    "        length = int(msg[:separator_index])\n",
    "        decoded_messages.append(\n",
    "            msg[separator_index + len(_MESSAGE_PREFIX):separator_index + len(_MESSAGE_PREFIX) + length])\n",
    "        msg = msg[separator_index + len(_MESSAGE_PREFIX) + length:]\n",
    "\n",
    "    events = []\n",
    "    for m in decoded_messages:\n",
    "        if m.startswith(\"~h~\"):\n",
    "            await _send(socket, m)\n",
    "        if m.startswith(\"{\"):\n",
    "            events.append(json.loads(m))\n",
    "    return events\n",
    "\n",
    "\n",
    "async def _init(socket: ClientConnection, tickers: list[str], data: dict[str, Any],\n",
    "                mode: Literal[\"quote\", \"bar\", \"all\"] = \"all\"):\n",
    "    qs_session = _gen_session_id(\"qs\")\n",
    "    cs_session = _gen_session_id(\"cs\")\n",
    "    keys = {f\"sds_sym_{i + 1}\": {\"t\": tickers[i], \"i\": i + 1} for i in range(len(tickers))}\n",
    "\n",
    "    # Store the key of the symbol that is completed\n",
    "    data['quotes'] = {}\n",
    "    data['bars'] = {}\n",
    "    data['bar_completed'] = 0\n",
    "    data['bar_started'] = []\n",
    "    data['quote_completed'] = 0\n",
    "    data['qs'] = qs_session\n",
    "    data['cs'] = cs_session\n",
    "    data['keys'] = keys\n",
    "\n",
    "    await _send(socket, {\"m\": \"set_auth_token\", \"p\": [\"unauthorized_user_token\"]})\n",
    "    await _send(socket, {\"m\": \"set_locale\", \"p\": [\"en\", \"IN\"]})\n",
    "\n",
    "    if mode == \"quote\":\n",
    "        data['bar_completed'] = len(tickers)\n",
    "\n",
    "    if mode == \"bar\":\n",
    "        data['quote_completed'] = len(tickers)\n",
    "\n",
    "    if mode == \"all\" or mode == \"quote\":\n",
    "        await _send(socket, {\"m\": \"quote_create_session\", \"p\": [qs_session]})\n",
    "        await _send(socket, {\"m\": \"quote_add_symbols\", \"p\": [qs_session, *tickers]})\n",
    "    if mode == \"all\" or mode == \"bar\":\n",
    "        await _send(socket, {\"m\": \"chart_create_session\", \"p\": [cs_session, \"\"]})\n",
    "        await _send(socket, {\"m\": \"switch_timezone\", \"p\": [cs_session, \"Asia/Kolkata\"]})\n",
    "        resolve_request = []\n",
    "        for symbol_key in keys:\n",
    "            meta = keys[symbol_key]\n",
    "            ticker = meta['t']\n",
    "            p = json.dumps({\"adjustment\": \"splits\", \"currency-id\": \"INR\", \"symbol\": ticker})\n",
    "            request = {\"m\": \"resolve_symbol\", \"p\": [cs_session, symbol_key, f'={p}']}\n",
    "            resolve_request.append(request)\n",
    "        await _send(socket, resolve_request)\n",
    "\n",
    "\n",
    "async def _end(socket: ClientConnection):\n",
    "    await socket.close()\n",
    "\n",
    "\n",
    "async def _send(socket: ClientConnection, data: str | dict[str, Any] | list[dict[str, Any]]):\n",
    "    message = _encode(data)\n",
    "    await socket.send(message)\n",
    "\n",
    "\n",
    "def _gen_session_id(prefix: str):\n",
    "    characters = ascii_letters + digits  # A-Z, a-z, 0-9\n",
    "    random_string = ''.join(choices(characters, k=12))\n",
    "    return f\"{prefix}_{random_string}\"\n",
    "\n",
    "\n",
    "async def _process_data(socket: ClientConnection, tickers: list[str], message: str | bytes, data: dict[str, Any]):\n",
    "    events = await _decode(socket, message)\n",
    "\n",
    "    for event in events:\n",
    "        event_type = event.get(\"m\")\n",
    "        if event_type == \"qsd\":\n",
    "            data['quotes'] = _on_qsd_event(event, data)\n",
    "        if event_type == \"quote_completed\":\n",
    "            completed_count = data['quote_completed']\n",
    "            data[\"quote_completed\"] = completed_count + 1\n",
    "        if event_type == \"symbol_resolved\":\n",
    "            await _on_symbol_resolved(socket, data)\n",
    "        if event_type == \"timescale_update\":\n",
    "            await _on_timescale_update(event, data)\n",
    "        if event_type == \"series_completed\":\n",
    "            await _on_series_completed(socket, tickers, data)\n",
    "\n",
    "    return data.get(\"quote_completed\", 0) == len(tickers) and data.get(\"bar_completed\", 0) == len(tickers)\n",
    "\n",
    "\n",
    "def _on_qsd_event(event: dict[str, Any], data: dict[str, Any]) -> dict[str, Any]:\n",
    "    quotes = data.get(\"quotes\", {})\n",
    "\n",
    "    q: dict = event.get(\"p\")[1]\n",
    "    ticker = q.get(\"n\")\n",
    "\n",
    "    if q.get(\"v\") is None:\n",
    "        return quotes\n",
    "\n",
    "    # Update Quote\n",
    "    ticker_quote = quotes.get(ticker, {})\n",
    "    q_data: dict = q.get(\"v\")\n",
    "    quotes[ticker] = ticker_quote | q_data\n",
    "\n",
    "    return quotes\n",
    "\n",
    "\n",
    "async def _on_symbol_resolved(socket: ClientConnection, data: dict[str, Any]):\n",
    "    symbol_resolve_count = data.get(\"symbol_resolve_count\", 0)\n",
    "    symbol_resolve_count = symbol_resolve_count + 1\n",
    "    data[\"symbol_resolve_count\"] = symbol_resolve_count\n",
    "\n",
    "    keys = data['keys']\n",
    "    ticker_count = len(keys.keys())\n",
    "    if symbol_resolve_count != ticker_count:\n",
    "        # All symbol not yet resolved\n",
    "        return\n",
    "\n",
    "    bar_started = data['bar_started']\n",
    "    to_start = list(set(keys.keys()) - set(bar_started))\n",
    "    if len(to_start) == 0:\n",
    "        return\n",
    "\n",
    "    # Start with the first pending\n",
    "    cs = data[\"cs\"]\n",
    "    symbol_key = to_start[0]\n",
    "    series_id = f\"s{keys[symbol_key]['i']}\"\n",
    "\n",
    "    # Request data\n",
    "    await _send(socket, {\"m\": \"create_series\", \"p\": [cs, \"sds_1\", series_id, symbol_key, \"1D\", 5500]})\n",
    "    bar_started.append(symbol_key)\n",
    "\n",
    "\n",
    "async def _on_timescale_update(event: dict[str, Any], data: dict[str, Any]):\n",
    "    p: dict[str, Any] = event.get(\"p\")[1]\n",
    "    series = p.get('sds_1')\n",
    "    if series is None or series.get(\"s\") is None:\n",
    "        print(\"Series is missing\", event)\n",
    "        return\n",
    "\n",
    "    # Day Data\n",
    "    d = list(map(lambda s: s['v'], series.get(\"s\")))\n",
    "\n",
    "    keys = data['keys']\n",
    "    bar_started = data['bar_started']\n",
    "    bars = data['bars']\n",
    "\n",
    "    # Mark the bars to loaded\n",
    "    last_bar_key = bar_started[-1]\n",
    "    ticker = keys[last_bar_key]['t']\n",
    "\n",
    "    bar = bars.get(ticker, [])\n",
    "    bars[ticker] = d + bar\n",
    "\n",
    "\n",
    "async def _on_series_completed(socket: ClientConnection, ticker: list[str], data: dict[str, Any]):\n",
    "    cs = data[\"cs\"]\n",
    "    keys = data['keys']\n",
    "    bar_started = data['bar_started']\n",
    "    bar_completed = data['bar_completed'] + 1\n",
    "    data['bar_completed'] = bar_completed\n",
    "\n",
    "    if bar_completed == len(ticker):\n",
    "        return\n",
    "\n",
    "    pending = list(set(keys.keys()) - set(bar_started))\n",
    "    if len(pending) == 0:\n",
    "        return\n",
    "\n",
    "    symbol_key = pending[0]\n",
    "    meta = keys[symbol_key]\n",
    "    series_id = f\"s{meta['i']}\"\n",
    "\n",
    "    await _send(socket, {\"m\": \"modify_series\", \"p\": [cs, \"sds_1\", series_id, symbol_key, \"1D\", \"\"]})\n",
    "    bar_started.append(symbol_key)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TtD3cqpOfCK-",
    "ExecuteTime": {
     "end_time": "2025-01-28T12:12:24.645008Z",
     "start_time": "2025-01-28T12:12:24.641468Z"
    }
   },
   "source": [
    "def get_all_symbols(market: str):\n",
    "    indexes = {\n",
    "        \"india\": [\n",
    "            \"NSE:NIFTY\", \"NSE:NIFTYJR\", \"NSE:CNX500\", \"NSE:BANKNIFTY\", \"NSE:CNXFINANCE\", \"NSE:CNXIT\",\n",
    "            \"NSE:CNXAUTO\", \"NSE:CNXPHARMA\", \"NSE:CNXPSUBANK\", \"NSE:CNXMETAL\", \"NSE:CNXFMCG\", \"NSE:CNXREALTY\",\n",
    "            \"NSE:CNXMEDIA\", \"NSE:CNXINFRA\", \"NSE:NIFTYPVTBANK\", \"NSE:NIFTY_OIL_AND_GAS\", \"NSE:NIFTY_HEALTHCARE\",\n",
    "            \"NSE:NIFTY_CONSR_DURBL\", \"NSE:CNX200\", \"NSE:NIFTY_MID_SELECT\",\n",
    "            \"NSE:CNXSMALLCAP\", \"NSE:CNXMIDCAP\", \"NSE:CNXENERGY\", \"NSE:NIFTYMIDCAP50\", \"NSE:NIFTYSMLCAP250\",\n",
    "            \"NSE:CNXPSE\", \"NSE:NIFTYMIDSML400\", \"NSE:NIFTYMIDCAP150\", \"NSE:CNXCONSUMPTION\", \"NSE:CNXCOMMODITIES\",\n",
    "            \"NSE:NIFTY_MICROCAP250\", \"NSE:CPSE\", \"NSE:CNXSERVICE\", \"NSE:CNXMNC\", \"NSE:CNX100\", \"NSE:NIFTYALPHA50\",\n",
    "            \"NSE:NIFTY_TOTAL_MKT\", \"NSE:NIFTY_INDIA_MFG\",\n",
    "            \"NSE:NIFTY_IND_DIGITAL\", \"NSE:NIFTY_LARGEMID250\", \"BSE:SENSEX\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    url = f\"https://scanner.tradingview.com/{market}/scan\"\n",
    "    payload = {\n",
    "        \"columns\": [],\n",
    "        \"filter\": [\n",
    "            {\n",
    "                \"left\": \"is_primary\",\n",
    "                \"operation\": \"equal\",\n",
    "                \"right\": True\n",
    "            }\n",
    "        ],\n",
    "        \"sort\": {\n",
    "            \"sortBy\": \"market_cap_basic\",\n",
    "            \"sortOrder\": \"desc\"\n",
    "        },\n",
    "    }\n",
    "    headers = {'Content-Type': 'text/plain'}\n",
    "\n",
    "    r = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload))\n",
    "    r.raise_for_status()\n",
    "    data = r.json()['data']  # [{'s': 'NYSE:HKD', 'd': []}, {'s': 'NASDAQ:ALTY', 'd': []}...]\n",
    "    return indexes[market] + [i['s'] for i in data]\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "t = get_all_symbols(\"india\")\n",
    "print(\"Ticker fetched\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "quotes, bars = await download(t)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7GtHPtXDJVpF",
    "ExecuteTime": {
     "end_time": "2025-01-28T12:14:05.291991Z",
     "start_time": "2025-01-28T12:14:05.287803Z"
    }
   },
   "source": [
    "len(quotes.keys())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GwTsWz2yfN4q",
    "ExecuteTime": {
     "end_time": "2025-01-28T12:14:05.444488Z",
     "start_time": "2025-01-28T12:14:05.398049Z"
    }
   },
   "source": [
    "q = to_quote_df(quotes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Quote DataFrames...\n",
      "Generated Quote DataFrames...\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JX9-UdpufT5H",
    "ExecuteTime": {
     "end_time": "2025-01-28T12:14:05.700685Z",
     "start_time": "2025-01-28T12:14:05.524216Z"
    }
   },
   "source": [
    "daily_candles = to_bars_df(bars)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bar DataFrames...\n",
      "Generated Bar DataFrames...\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yMM2b7_fWcx"
   },
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZhAxmlDfa9V"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43pB13hgfXSq"
   },
   "outputs": [],
   "source": [
    "# Function to process each row\n",
    "def calculate_surprise(value):\n",
    "    if isinstance(value, list):  # Only process if the value is a list of dicts\n",
    "        # Create a temporary DataFrame\n",
    "        temp_df = pd.DataFrame(value)\n",
    "        # Ensure the required keys are present\n",
    "        if 'Estimate' not in temp_df.columns:\n",
    "            temp_df['Estimate'] = None\n",
    "        if 'Actual' not in temp_df.columns:\n",
    "            temp_df['Actual'] = None\n",
    "        # Add the Surprise column\n",
    "        temp_df['Surprise'] = (temp_df['Actual'] - temp_df['Estimate']) / temp_df['Estimate'] * 100\n",
    "        # Convert back to a list of dicts\n",
    "        temp_df = temp_df.where(pd.notnull(temp_df), None)\n",
    "        temp_df = temp_df.replace([np.nan, np.inf, -np.inf], None)\n",
    "\n",
    "        return temp_df.to_dict(orient='records')\n",
    "    elif pd.isna(value):  # Replace NaN with an empty list\n",
    "        return []\n",
    "    else:\n",
    "        return value  # Return as-is for other types\n",
    "\n",
    "\n",
    "def get_surprise_df(s: pd.Series, n: int, prefix: str, index: pd.Series):\n",
    "    # Extract the latest n Surprise values and create individual columns\n",
    "    def extract_latest_surprise(value):\n",
    "        if isinstance(value, list):\n",
    "            # Extract the Surprise values, reverse the order for latest first\n",
    "            surprise_values = [d.get('Surprise', None) for d in value if d.get(\"IsReported\", False)][-n:]\n",
    "            # Fill the list to ensure exactly `n` values\n",
    "            return list(reversed(surprise_values + [None] * (n - len(surprise_values))))\n",
    "        return [None] * n  # Return empty if not a list\n",
    "\n",
    "    surprise_columns = [f\"{prefix}_{i}\" for i in range(n)]\n",
    "    surprises_values = s.map(lambda x: extract_latest_surprise(x)).tolist()\n",
    "    return pd.DataFrame(surprises_values, index=index, columns=surprise_columns, dtype='float32')\n",
    "\n",
    "\n",
    "def flatten_list(s: pd.Series, n: int, prefix: str):\n",
    "    cols = {}\n",
    "    s_normalized = s.apply(lambda x: x if isinstance(x, list) else []).astype(object)\n",
    "    cols[f'{prefix}_h'] = s_normalized\n",
    "\n",
    "    for i in range(n):\n",
    "        col = f'{prefix}_{i}'\n",
    "        cols[col] = s_normalized.map(lambda x: x[i] if i < len(x) else None).astype(float)\n",
    "\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def growth(d: pd.DataFrame, comp: str, n: int, name: str, period: int = 1):\n",
    "    cols = {}\n",
    "    for i in range(n - period):\n",
    "        curr_col = f\"{comp}_{i}\"\n",
    "        prev_col = f\"{comp}_{i + period}\"\n",
    "        col = f\"{name}_{i}\"\n",
    "        cols[col] = ((d[curr_col] - d[prev_col]) / d[prev_col] * 100).astype('float32')\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def forecast_growth(d: pd.DataFrame, l_rep: str, comp: str, n: int, name: str, loopback: int = 1):\n",
    "    cols = {}\n",
    "    for i in range(min(n - loopback, 4)):\n",
    "        curr_col = f\"{comp}_{i}\"\n",
    "        prev_col = l_rep if i == 0 else f\"{comp}_{i - 1}\"\n",
    "        col = f\"{name}_{i}\"\n",
    "        cols[col] = ((d[curr_col] - d[prev_col]) / d[prev_col] * 100).astype('float32')\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def growth_avg(n: int, comp: str, name: str):\n",
    "    cols = {}\n",
    "    for i in range(2, min(4, n)):\n",
    "        needed_cols = [f\"{comp}_{r}\" for r in range(i)]\n",
    "        col = f\"{name}_{i}\"\n",
    "        cols[col] = df[needed_cols].mean(axis=1, skipna=True)\n",
    "    return pd.DataFrame(cols)\n",
    "\n",
    "\n",
    "def to_datetime(series: pd.Series, unit='s', utc=True):\n",
    "    return pd.to_datetime(series.astype('Int64'), unit=unit)\n",
    "\n",
    "\n",
    "def to_weekly_candles(d: pd.DataFrame):\n",
    "    # TODO: Try with .resample('W-MON', label='left', closed='left')\n",
    "\n",
    "    d = d.copy()\n",
    "    # Step 1: Adjust the timestamp index to the start of the week (Monday 12:00 AM UTC)\n",
    "    d[\"Week_Start\"] = d.index.to_period(\"W\").start_time\n",
    "\n",
    "    # Step 2: Group by the week start\n",
    "    w: pd.DataFrame = (d.groupby(\"Week_Start\").agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }).reset_index())\n",
    "\n",
    "    w = w.rename(columns={'Week_Start': 'timestamp'}).set_index('timestamp')\n",
    "    return w\n",
    "\n",
    "\n",
    "def to_monthly_candles(d: pd.DataFrame):\n",
    "    d = d.copy()\n",
    "    # Step 1: Adjust the timestamp index to the start of the week (Monday 12:00 AM UTC)\n",
    "    d[\"Month_Start\"] = d.index.to_period(\"M\").start_time\n",
    "\n",
    "    # Step 2: Group by the week start\n",
    "    m = (d.groupby(\"Month_Start\").agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }).reset_index())\n",
    "\n",
    "    m = m.rename(columns={'Month_Start': 'timestamp'}).set_index('timestamp')\n",
    "    return m\n",
    "\n",
    "\n",
    "def to_yearly_candles(m: pd.DataFrame):\n",
    "    return m.resample('YS').agg({\n",
    "        'open': 'first',  # First Open in the year\n",
    "        'high': 'max',  # Maximum High in the year\n",
    "        'low': 'min',  # Minimum Low in the year\n",
    "        'close': 'last',  # Last Close in the year\n",
    "        'volume': 'sum'  # Sum of Volume in the year\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZnzWyaBffRE"
   },
   "source": [
    "## Fundamental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le165GGjfdWj"
   },
   "outputs": [],
   "source": [
    "fq_count = 12\n",
    "fy_count = 4\n",
    "today = pd.Timestamp.today()\n",
    "df = pd.DataFrame()\n",
    "df['ticker'] = q.index.astype(str)\n",
    "df = df.set_index(['ticker'])\n",
    "######################################## ~~~START~~~~ ########################################\n",
    "\n",
    "\n",
    "########################### ~~~General~~~~ ###########################\n",
    "df['isin'] = q['isin'].astype(str)\n",
    "df['name'] = q['short_name'].astype(str)\n",
    "df['is_primary_listing'] = q['is-primary-listing'].astype(bool)\n",
    "df['logo'] = q['logoid'].astype(str)\n",
    "df['description'] = q['description'].astype(str)\n",
    "df['type'] = q['type'].astype('category')\n",
    "df['exchange'] = (q['source-id'].fillna(q['exchange']).fillna(q['exchange-listed']).astype('category'))\n",
    "df['exchange_logo'] = (q['source-logoid'].astype('category'))\n",
    "df['exchange_logo'] = df[\"exchange_logo\"].where(\n",
    "    df[\"exchange_logo\"].notna(),\n",
    "    df.groupby(\"exchange\", observed=True)[\"exchange_logo\"].transform(\"first\")\n",
    ")\n",
    "df['timezone'] = q['timezone'].astype('category')\n",
    "df['currency'] = (q['currency-id']\n",
    "                  .fillna(q['currency_id'])\n",
    "                  .fillna(q['currency_code'])\n",
    "                  .fillna(q['currency_fund'])\n",
    "                  .fillna(q['currency'])\n",
    "                  .astype('category'))\n",
    "df['currency_logo'] = q['currency-logoid']\n",
    "df['fundamental_currency'] = (q['fundamental_currency_code'].fillna(df['currency']).astype('category'))\n",
    "df['subsessions'] = q['subsessions']\n",
    "df['session_holidays'] = q['session_holidays']\n",
    "########################### ~~~General~~~~ ###########################\n",
    "\n",
    "\n",
    "########################### ~~~Fiscal~~~~ ###########################\n",
    "df['fiscal_period_fy'] = q['fiscal_period_fy'].astype('category')\n",
    "df['fiscal_period_end_fy'] = to_datetime(q['fiscal_period_end_fy'])\n",
    "df['fiscal_period_fq'] = q['fiscal_period_fq'].astype('category')\n",
    "df['fiscal_period_end_fq'] = to_datetime(q['fiscal_period_end_fq'])\n",
    "df['fiscal_period_fy_h'] = q['fiscal_period_fy_h']\n",
    "df['fiscal_period_end_fy_h'] = q['fiscal_period_end_fy_h']\n",
    "df['fiscal_period_fq_h'] = q['fiscal_period_fq_h']\n",
    "df['fiscal_period_end_fq_h'] = q['fiscal_period_end_fq_h']\n",
    "########################### ~~~Fiscal~~~~ ###########################\n",
    "\n",
    "\n",
    "########################### ~~~Company~~~~ ###########################\n",
    "df['fundamental_data'] = q['fundamental_data']\n",
    "df['sector'] = q['sector'].astype('category')\n",
    "df['group'] = q['group'].astype('category')\n",
    "df['industry'] = q['industry'].astype('category')\n",
    "df['sub_industry'] = None\n",
    "df['sub_industry'] = df[\"sub_industry\"].astype('category')\n",
    "df['logo'] = q['logoid'].astype(str)\n",
    "df['ceo'] = q['ceo'].astype(str)\n",
    "df['website'] = q['web_site_url'].astype(str)\n",
    "df['country'] = q['country'].astype('category')\n",
    "df['location'] = q['location'].astype(str)\n",
    "df['country_code'] = q['country_code'].astype('category')\n",
    "df['employees'] = q['number_of_employees'].astype('Int64')\n",
    "df['business_description'] = q['business_description'].astype(str)\n",
    "df['ipo_date'] = to_datetime(q['first_bar_time_1d'])\n",
    "df['most_recent_split'] = to_datetime(q['split_last_date'])\n",
    "df['mcap'] = q['market_cap_basic'].astype(float)\n",
    "df['shares_float'] = q['float_shares_outstanding'].astype(float)\n",
    "df['total_shares_outstanding'] = q['total_shares_outstanding'].astype(float)\n",
    "########################### ~~~Company~~~~ ###########################\n",
    "\n",
    "\n",
    "########################### ~~~Revenue~~~~ ###########################\n",
    "df['price_revenue_ttm'] = q['price_revenue_ttm']\n",
    "df['revenue_action_fq_h'] = q['revenues_fq_h'].map(calculate_surprise)\n",
    "df['revenue_action_fy_h'] = q['revenues_fy_h'].map(calculate_surprise)\n",
    "# Add Surprise FQ\n",
    "df = pd.concat([df, get_surprise_df(df['revenue_action_fq_h'], fq_count, prefix=\"revenue_surprise_fq\", index=df.index)],\n",
    "               axis=1)\n",
    "# Add Surprise FY\n",
    "df = pd.concat([df, get_surprise_df(df['revenue_action_fy_h'], fy_count, prefix=\"revenue_surprise_fy\", index=df.index)],\n",
    "               axis=1)\n",
    "# Reported Revenue FQ\n",
    "df = pd.concat([df, flatten_list(q['revenue_fq_h'], fq_count, prefix=\"revenue_fq\")], axis=1)\n",
    "# Reported Revenue FY\n",
    "df = pd.concat([df, flatten_list(q['revenue_fy_h'], fy_count, prefix=\"revenue_fy\")], axis=1)\n",
    "# Forecasted Revenue FQ\n",
    "df = pd.concat([df, flatten_list(q['revenue_forecast_fq_h'], fq_count, prefix=\"revenue_forecast_fq\")], axis=1)\n",
    "# Forecasted Revenue FY\n",
    "df = pd.concat([df, flatten_list(q['revenue_forecast_fy_h'], fy_count, prefix=\"revenue_forecast_fy\")], axis=1)\n",
    "# Reported Revenue Growth FQ\n",
    "df = pd.concat([df, growth(df, comp='revenue_fq', n=fq_count, name='revenue_growth_fq')], axis=1)\n",
    "# Reported Revenue Growth YOY FQ\n",
    "df = pd.concat([df, growth(df, comp='revenue_fq', n=fq_count, name='revenue_growth_yoy_fq', period=4)], axis=1)\n",
    "# Reported Revenue Growth FY\n",
    "df = pd.concat([df, growth(df, comp='revenue_fy', n=fy_count, name='revenue_growth_fy')], axis=1)\n",
    "# Report Average Revenue Growth FQ\n",
    "df = pd.concat([df, growth_avg(n=fq_count, comp='revenue_growth_fq', name='revenue_avg_growth_fq')], axis=1)\n",
    "# Report Average Revenue Growth FY\n",
    "df = pd.concat([df, growth_avg(n=fy_count, comp='revenue_growth_fy', name='revenue_avg_growth_fy')], axis=1)\n",
    "# Forecast Revenue Growth FQ\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    forecast_growth(df, n=fq_count, l_rep='revenue_fq_0', comp='revenue_forecast_fq', name='revenue_forecast_growth_fq')\n",
    "], axis=1)\n",
    "# Forecast Revenue Growth FY\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    forecast_growth(df, n=fy_count, l_rep='revenue_fy_0', comp='revenue_forecast_fy', name='revenue_forecast_growth_fy')\n",
    "], axis=1)\n",
    "# Forecast Average Revenue Growth FQ\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    growth_avg(n=fq_count, comp='revenue_forecast_growth_fq', name='revenue_forecast_growth_avg_fq')\n",
    "], axis=1)\n",
    "# Forecast Average Revenue Growth FY\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    growth_avg(n=fy_count, comp='revenue_forecast_growth_fy', name='revenue_forecast_growth_avg_fy')\n",
    "], axis=1)\n",
    "########################### ~~~Revenue~~~~ ###########################\n",
    "\n",
    "########################### ~~~Earning~~~~ ###########################\n",
    "#All Earning Release\n",
    "df['earnings_release_date_fq_h'] = q['earnings_release_date_fq_h']  #ABS Date\n",
    "df['earnings_release_date_fy_h'] = q['earnings_release_date_fy_h']  #ABD Date\n",
    "df['earnings_fiscal_period_fq_h'] = q['earnings_fiscal_period_fq_h']  # Period\n",
    "df['earnings_fiscal_period_fy_h'] = q['earnings_fiscal_period_fy_h']  # Period\n",
    "\n",
    "# Latest Earning Date\n",
    "df['earnings_release_date'] = to_datetime(q['earnings_release_date'])\n",
    "df['earnings_release_time'] = q['earnings_release_time'] == 1\n",
    "# Latest FQ Earning Release Date\n",
    "df['earnings_release_date_fq'] = to_datetime(q['earnings_release_date_fq'])\n",
    "df['earnings_release_time_fq'] = q['earnings_release_time_fq'] == 1\n",
    "# Latest FQ Earning Release Date\n",
    "df['earnings_release_date_fy'] = to_datetime(q['earnings_release_date_fy'])\n",
    "# Next Latest Earning Date\n",
    "df['earnings_release_next_date'] = to_datetime(q['earnings_release_next_date'])\n",
    "df['earnings_release_next_time'] = q['earnings_release_next_time'] == 1\n",
    "# Next FQ Earning Release Date\n",
    "df['earnings_release_next_date_fq'] = to_datetime(q['earnings_release_next_date_fq'])\n",
    "df['earnings_release_next_time_fq'] = q['earnings_release_next_time_fq'] == 1\n",
    "# Next FY Earning Release Date\n",
    "df['earnings_release_next_date_fy'] = to_datetime(q['earnings_release_next_date_fy'])\n",
    "# Latest Earning Release Trading Date\n",
    "df['earnings_release_trading_date_fq'] = to_datetime(q['earnings_release_trading_date_fq'])\n",
    "df['earnings_release_trading_date_fy'] = to_datetime(q['earnings_release_trading_date_fy'])\n",
    "df['earnings_release_trading_date'] = df[['earnings_release_trading_date_fq', 'earnings_release_trading_date_fy']].max(\n",
    "    axis=1)\n",
    "# Latest Next Earning Release Trading Date\n",
    "df['earnings_release_next_trading_date_fq'] = to_datetime(q['earnings_release_next_trading_date_fq'])\n",
    "df['earnings_release_next_trading_date_fy'] = to_datetime(q['earnings_release_next_trading_date_fy'])\n",
    "df['earnings_release_next_trading_date'] = df[\n",
    "    ['earnings_release_next_trading_date_fq', 'earnings_release_next_trading_date_fy']].min(axis=1)\n",
    "\n",
    "df['earning_action_fq_h'] = q['earnings_fq_h'].map(calculate_surprise)\n",
    "df['earning_action_fy_h'] = q['earnings_fy_h'].map(calculate_surprise)\n",
    "df['earnings_per_share_ttm'] = q['earnings_per_share_ttm'].astype(float)\n",
    "\n",
    "# Add Surprise FQ\n",
    "df = pd.concat([df, get_surprise_df(df['earning_action_fq_h'], fq_count, prefix=\"earning_surprise_fq\", index=df.index)],\n",
    "               axis=1)\n",
    "# Add Surprise FY\n",
    "df = pd.concat([df, get_surprise_df(df['earning_action_fy_h'], fy_count, prefix=\"earning_surprise_fy\", index=df.index)],\n",
    "               axis=1)\n",
    "# Reported Earning FQ\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_diluted_fq_h'], fq_count, prefix=\"eps_fq\")], axis=1)\n",
    "# Reported Earning FY\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_diluted_fy_h'], fy_count, prefix=\"eps_fy\")], axis=1)\n",
    "# Reported Earning TTM\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_diluted_ttm_h'], fy_count, prefix=\"eps_ttm\")], axis=1)\n",
    "# Estimated Earning FQ\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_forecast_fq_h'], fq_count, prefix=\"eps_estimated_fq\")], axis=1)\n",
    "# Estimated Earning FY\n",
    "df = pd.concat([df, flatten_list(q['earnings_per_share_forecast_fy_h'], fy_count, prefix=\"eps_estimated_fy\")], axis=1)\n",
    "# Reported Earning Growth FQ\n",
    "df = pd.concat([df, growth(df, comp='eps_fq', n=fq_count, name='eps_growth_fq')], axis=1)\n",
    "# Reported Earning Growth YOY FQ\n",
    "df = pd.concat([df, growth(df, comp='eps_fq', n=fq_count, name='eps_growth_yoy_fq', period=4)], axis=1)\n",
    "# Reported Earning Growth FY\n",
    "df = pd.concat([df, growth(df, comp='eps_fy', n=fy_count, name='eps_growth_fy')], axis=1)\n",
    "# Reported Earning Growth TTM\n",
    "df = pd.concat([df, growth(df, comp='eps_ttm', n=fy_count, name='eps_growth_ttm')], axis=1)\n",
    "# Reported Average Revenue Growth FQ\n",
    "df = pd.concat([df, growth_avg(n=fq_count, comp='eps_growth_fq', name='eps_avg_growth_fq')], axis=1)\n",
    "# Report Average Revenue Growth FY\n",
    "df = pd.concat([df, growth_avg(n=fy_count, comp='eps_growth_fy', name='eps_avg_growth_fy')], axis=1)\n",
    "# Estimated Earning Growth FQ\n",
    "df = pd.concat([df, forecast_growth(df, n=fq_count, l_rep='eps_fq_0', comp='eps_estimated_fq',\n",
    "                                    name='eps_estimated_growth_fq')], axis=1)\n",
    "# Estimated Earning Growth FY\n",
    "df = pd.concat([df, forecast_growth(df, n=fy_count, l_rep='eps_fy_0', comp='eps_estimated_fy',\n",
    "                                    name='eps_estimated_growth_fy')], axis=1)\n",
    "# Estimated Earning Growth Average FQ\n",
    "df = pd.concat([df, growth_avg(n=fq_count, comp='eps_estimated_growth_fq', name='eps_estimated_growth_avg_fq')], axis=1)\n",
    "# Estimated Earning Growth Average FY\n",
    "df = pd.concat([df, growth_avg(n=fy_count, comp='eps_estimated_growth_fy', name='eps_estimated_growth_avg_fy')], axis=1)\n",
    "# Net Income TTM\n",
    "df = pd.concat([df, flatten_list(q['net_income_ttm_h'], fq_count, prefix=\"net_income_ttm\")], axis=1)\n",
    "# Net Income FQ\n",
    "df = pd.concat([df, flatten_list(q['net_income_fq_h'], fq_count, prefix=\"net_income_fq\")], axis=1)\n",
    "# Net Income FY\n",
    "df = pd.concat([df, flatten_list(q['net_income_fy_h'], fq_count, prefix=\"net_income_fy\")], axis=1)\n",
    "########################### ~~~Earning~~~~ ###########################\n",
    "\n",
    "########################### ~~~Balance Sheet~~~~ ###########################\n",
    "# Total Asset FQ\n",
    "df = pd.concat([df, flatten_list(q['total_assets_fq_h'], fq_count, prefix=\"total_assets_fq\")], axis=1)\n",
    "\n",
    "# Total Liability FY\n",
    "df = pd.concat([df, flatten_list(q['total_liabilities_fq_h'], fq_count, prefix=\"total_liabilities_fq\")], axis=1)\n",
    "\n",
    "# Total Asset FY\n",
    "df = pd.concat([df, flatten_list(q['total_assets_fy_h'], fq_count, prefix=\"total_assets_fy\")], axis=1)\n",
    "\n",
    "# Total Liability FY\n",
    "df = pd.concat([df, flatten_list(q['total_liabilities_fy_h'], fq_count, prefix=\"total_liabilities_fy\")], axis=1)\n",
    "\n",
    "########################### ~~~Dividend~~~~ ###########################\n",
    "\n",
    "df['dividend_amount'] = q['dividend_amount_recent'].astype(float)\n",
    "df['divided_ex_date'] = to_datetime(q['dividend_ex_date_recent'])\n",
    "df['divided_payment_date'] = to_datetime(q['dividend_payment_date_recent'])\n",
    "df['dividend_yield'] = q['dividends_yield_current'].astype('float16')\n",
    "df = pd.concat([df, flatten_list(q['dividends_yield_fy_h'], fq_count, prefix=\"dividends_yield_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['dividend_payout_ratio_fq_h'], fq_count, prefix=\"dividend_payout_ratio_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['dividend_payout_ratio_fy_h'], fq_count, prefix=\"dividend_payout_ratio_fy\")], axis=1)\n",
    "\n",
    "########################### ~~~Dividend~~~~ ###########################\n",
    "\n",
    "######################## ~~~Extra~~~~ ########################\n",
    "df['price_earnings_ttm'] = q['price_earnings_ttm']\n",
    "df['price_revenue_ttm'] = q['price_revenue_ttm']\n",
    "df['price_sales_ttm'] = q['price_sales_ttm']\n",
    "df['price_earnings_growth_ttm'] = q['price_earnings_growth_ttm']\n",
    "df['current_ratio'] = q['current_ratio']\n",
    "df['price_earnings_run_rate'] = q['lp'] / q['earnings_per_share_fq'] * 4\n",
    "df['forward_price_earnings'] = q['lp'] / q['earnings_per_share_forecast_fy']\n",
    "\n",
    "df = pd.concat([df, flatten_list(q['price_earnings_fy_h'], fq_count, prefix=\"price_earnings_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_earnings_fq_h'], fq_count, prefix=\"price_earnings_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['current_ratio_fq_h'], fq_count, prefix=\"current_ratio_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['current_ratio_fy_h'], fq_count, prefix=\"current_ratio_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['debt_to_equity_fq_h'], fq_count, prefix=\"debt_to_equity_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['debt_to_equity_fy_h'], fq_count, prefix=\"debt_to_equity_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_book_fq_h'], fq_count, prefix=\"price_book_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_book_fy_h'], fq_count, prefix=\"price_book_fy\")], axis=1)\n",
    "# TODO Fix: Causing Parquet file serialization error\n",
    "# df = pd.concat([df, flatten_list(q['price_sales_fq_h'], fq_count, prefix=\"price_sales_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['price_sales_fy_h'], fq_count, prefix=\"price_sales_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebit_fq_h'], fq_count, prefix=\"ebit_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebit_fy_h'], fy_count, prefix=\"ebit_fy\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebitda_fq_h'], fq_count, prefix=\"ebitda_fq\")], axis=1)\n",
    "df = pd.concat([df, flatten_list(q['ebitda_fy_h'], fy_count, prefix=\"ebitda_fy\")], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ln03nxGfkqT"
   },
   "source": [
    "## Technical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4JGO1gdfla0"
   },
   "outputs": [],
   "source": [
    "def get_latest(value, index: int = -1):\n",
    "    if value is None:\n",
    "        return np.nan\n",
    "    if isinstance(value, pd.Series):\n",
    "        if index < 0 and abs(index) > len(value):\n",
    "            return np.nan\n",
    "        if index >= 0 and index >= len(value):\n",
    "            return np.nan\n",
    "        return value.iloc[index]\n",
    "    return value\n",
    "\n",
    "\n",
    "def ohlcv(candle: pd.DataFrame, name: str, shift: int = 0) -> dict[str, pd.Series]:\n",
    "    return {\n",
    "        f\"{name}_open\": candle.open if shift == 0 else candle.open.shift(shift),\n",
    "        f\"{name}_high\": candle.high if shift == 0 else candle.high.shift(shift),\n",
    "        f\"{name}_low\": candle.low if shift == 0 else candle.low.shift(shift),\n",
    "        f\"{name}_close\": candle.close if shift == 0 else candle.close.shift(shift),\n",
    "        f\"{name}_volume\": candle.volume if shift == 0 else candle.volume.shift(shift),\n",
    "    }\n",
    "\n",
    "\n",
    "def vwap(candle: pd.DataFrame, name: str) -> dict[str, pd.Series]:\n",
    "    v = (candle.high + candle.low + candle.close) / 3\n",
    "    away = (candle.close - v) / v * 100\n",
    "    return {\n",
    "        f'{name}_vwap': v,\n",
    "        f'away_from_{name}_vwap_pct': away,\n",
    "        f'price_above_{name}_vwap': candle.close > v,\n",
    "    }\n",
    "\n",
    "\n",
    "def price_change_close(candle: pd.DataFrame, periods: list[int] | range, name: str) -> dict[str, pd.Series]:\n",
    "    return {\n",
    "        f\"price_change_pct_{i}{name}\": (candle.close.pct_change(periods=i, fill_method=None) * 100) for i in periods\n",
    "    }\n",
    "\n",
    "\n",
    "def sma(series: pd.Series, periods: list[int] | range, name: str, freq: str, compare=False, relative=False,\n",
    "        run_rate=False) -> dict[\n",
    "    str, pd.Series]:\n",
    "    def to_key(i: int):\n",
    "        return f\"{name}_sma_{i}{freq}\"\n",
    "\n",
    "    def to_compare_key(i: int):\n",
    "        return f\"{name}_vs_{name}_sma_{i}{freq}\"\n",
    "\n",
    "    def to_relative_key(i: int):\n",
    "        return f\"relative_{name}_{i}{freq}\"\n",
    "\n",
    "    def to_run_rate(i: int):\n",
    "        return f\"run_rate_{name}_{i}{freq}\"\n",
    "\n",
    "    cols = {\n",
    "        to_key(i): series.rolling(i).mean() for i in periods\n",
    "    }\n",
    "\n",
    "    if compare:\n",
    "        cols = cols | {\n",
    "            to_compare_key(i): (series - cols[to_key(i)]) / cols[to_key(i)] * 100 for i in periods\n",
    "        }\n",
    "\n",
    "    if relative:\n",
    "        cols = cols | {\n",
    "            to_relative_key(i): series / cols[to_key(i)] for i in periods\n",
    "        }\n",
    "\n",
    "    if run_rate:\n",
    "        cols = cols | {\n",
    "            to_run_rate(i): series / cols[to_key(i)] * 100 for i in periods\n",
    "        }\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def ema(series: pd.Series, periods: list[int] | range, name: str, freq: str, compare=False) -> dict[str, pd.Series]:\n",
    "    return {\n",
    "        f\"{name}_ema_{i}{freq}\": series.ewm(i).mean() for i in periods\n",
    "    }\n",
    "\n",
    "\n",
    "def up_down(candle: pd.DataFrame, freq: str, period: list[int]) -> dict[str, pd.Series]:\n",
    "    cols: dict[str, pd.Series] = {}\n",
    "    for i in period:\n",
    "        c = candle.tail(i)\n",
    "        change = c.close - c.open\n",
    "        up = c[change > 0].volume.sum()\n",
    "        down = c[change < 0].volume.sum()\n",
    "        cols[f\"up_down_day_{i}{freq}\"] = up / down if down != 0 else np.nan\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def gap(candle: pd.DataFrame, freq: str):\n",
    "    prev_close = candle.close.shift(1)\n",
    "    gap_dollar = candle.open - prev_close\n",
    "    gap_pct = gap_dollar / prev_close * 100\n",
    "    unfilled_gap_dollar = ((candle.low.where(candle.low > prev_close, other=np.nan) - prev_close)\n",
    "                           .where(candle.high < prev_close, candle.high - prev_close))\n",
    "\n",
    "    unfilled_gap_pct = unfilled_gap_dollar / prev_close * 100\n",
    "    return {\n",
    "        f\"gap_dollar_{freq}\": gap_dollar,\n",
    "        f\"unfilled_gap_{freq}\": unfilled_gap_dollar,\n",
    "        f\"gap_pct_{freq}\": gap_pct,\n",
    "        f\"unfilled_gap_pct_{freq}\": unfilled_gap_pct\n",
    "    }\n",
    "\n",
    "\n",
    "def price_action(\n",
    "        d: pd.DataFrame,\n",
    "        d_w52: pd.DataFrame,\n",
    "        w: pd.DataFrame,\n",
    "        m: pd.DataFrame,\n",
    "        y: pd.DataFrame,\n",
    "        days_since_earning: int,\n",
    "        last_trading_day: pd.Timestamp,\n",
    "        daily_periods: list[int],\n",
    "        weekly_periods: list[int],\n",
    ") -> dict[str, pd.Series]:\n",
    "    high_52_week = d_w52.high.max()\n",
    "    low_52_week = d_w52.low.min()\n",
    "    all_time_high = d.high.max()\n",
    "    all_time_low = d.low.min()\n",
    "    earning_open = d.open.shift(days_since_earning)\n",
    "\n",
    "    #VWAP\n",
    "    cols = vwap(d, 'daily') | vwap(w, 'weekly') | vwap(m, 'monthly') | vwap(y, 'yearly')\n",
    "\n",
    "    # Price Change\n",
    "    cols = cols | price_change_close(d, [1, 2, 3, 4], 'D') | price_change_close(w, range(1, 4), 'W')\n",
    "    cols = cols | price_change_close(m, range(1, 12), 'M') | price_change_close(y, range(1, 5), 'Y')\n",
    "\n",
    "    # Price Change Comparison\n",
    "    cols = cols | {\"price_change_today_pct\": cols['price_change_pct_1D']}\n",
    "    cols = cols | {\"price_change_prev_week_close_pct\": cols['price_change_pct_1W'], }\n",
    "\n",
    "    # SMA\n",
    "    cols = cols | sma(d.close, daily_periods, 'price', 'D', compare=True)\n",
    "    cols = cols | sma(w.close, weekly_periods, 'price', 'W', compare=True)\n",
    "\n",
    "    #High Low\n",
    "    cols = cols | {\n",
    "        \"high_52_week\": high_52_week,\n",
    "        \"low_52_week\": low_52_week,\n",
    "        \"high_52_week_today\": d_w52.high.idxmax() == last_trading_day,\n",
    "        \"low_52_week_today\": d_w52.low.idxmax() == last_trading_day,\n",
    "        \"away_from_52_week_high_pct\": (d_w52.close - high_52_week) / high_52_week * 100,\n",
    "        \"away_from_52_week_low_pct\": (d_w52.close - low_52_week) / low_52_week * 100,\n",
    "        \"all_time_high\": all_time_high,\n",
    "        \"all_time_low\": all_time_low,\n",
    "        \"all_time_high_today\": d.high.idxmax() == last_trading_day,\n",
    "        \"all_time_low_today\": d.low.idxmax() == last_trading_day,\n",
    "        \"away_from_all_time_high_pct\": (d.close - all_time_high) / all_time_high * 100,\n",
    "        \"away_from_all_time_low_pct\": (d.close - all_time_low) / all_time_low * 100,\n",
    "    }\n",
    "\n",
    "    # Recent Price Change Comparison abs\n",
    "    cols = cols | {\n",
    "        \"price_change_today_abs\": d.close - d.close.shift(1),\n",
    "        \"price_change_from_open_abs\": d.close - d.open,\n",
    "        \"price_change_from_high_abs\": d.close - d.high,\n",
    "        \"price_change_from_low_abs\": d.close - d.low,\n",
    "    }\n",
    "\n",
    "    # Recent Price Change Comparison PCT\n",
    "    cols = cols | {\n",
    "        \"price_change_from_open_pct\": cols['price_change_from_open_abs'] / d.open * 100,\n",
    "        \"price_change_from_high_pct\": cols['price_change_from_high_abs'] / d.high * 100,\n",
    "        \"price_change_from_low_pct\": cols['price_change_from_low_abs'] / d.low * 100,\n",
    "        \"price_change_curr_week_open_pct\": (w.close - w.open) / w.open * 100,\n",
    "        \"price_change_since_earning_pct\": (d.close - earning_open) / earning_open * 100,\n",
    "    }\n",
    "\n",
    "    # Closing Range\n",
    "    cols = cols | {\n",
    "        \"dcr\": ((d.close - d.low) / (d.high - d.low)) * 100,\n",
    "        \"wcr\": ((w.close - w.low) / (w.high - w.low)) * 100,\n",
    "        \"mcr\": ((m.close - m.low) / (m.high - m.low)) * 100,\n",
    "    }\n",
    "\n",
    "    # Gaps\n",
    "    cols = cols | gap(d, \"D\") | gap(w, \"W\") | gap(m, \"M\")\n",
    "\n",
    "    # Up/Down\n",
    "    cols = cols | up_down(d, \"D\", [20, 50])\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def volume_action(\n",
    "        d: pd.DataFrame,\n",
    "        d_w52: pd.DataFrame,\n",
    "        d_since_earning: pd.DataFrame,\n",
    "        w: pd.DataFrame,\n",
    "        row,\n",
    "        daily_periods: list[int],\n",
    "        weekly_periods: list[int],\n",
    "        last_trading_day\n",
    "):\n",
    "    cols = {\n",
    "        \"highest_vol_since_earning\": False if len(\n",
    "            d_since_earning.volume) == 0 else d_since_earning.volume.idxmax() == last_trading_day,\n",
    "        \"highest_vol_in_1_year\": False if len(d_w52.volume) == 0 else d_w52.volume.idxmax() == last_trading_day,\n",
    "        \"highest_vol_ever\": False if len(d.volume) == 0 else d.volume.idxmax() == last_trading_day,\n",
    "        \"vol_vs_yesterday_vol\": d.volume.pct_change(periods=1, fill_method=None) * 100,\n",
    "        \"week_vol_vs_prev_week_vol\": w.volume.pct_change(periods=1, fill_method=None) * 100,\n",
    "    }\n",
    "\n",
    "    # SMA\n",
    "    cols = cols | sma(d.volume, daily_periods, 'vol', 'D', compare=True, relative=True, run_rate=True)\n",
    "    cols = cols | sma(w.volume, weekly_periods, 'vol', 'W', compare=True, relative=True, run_rate=True)\n",
    "\n",
    "    # Price Volume\n",
    "    price_volume = d.close * d.volume\n",
    "    cols = cols | {\"price_volume\": price_volume}\n",
    "    cols = cols | sma(price_volume, daily_periods, 'price_volume', 'D')\n",
    "\n",
    "    # Float Turnover\n",
    "    total_float = row.shares_float\n",
    "    float_turnover = d.volume / total_float * 100\n",
    "    cols = cols | {\"float_turnover\": float_turnover}\n",
    "    cols = cols | sma(float_turnover, daily_periods, 'float_turnover', 'D', compare=True)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def price_compare(d: pd.DataFrame):\n",
    "    prev = d.shift(1)\n",
    "    prev_high = prev.high\n",
    "    prev_low = prev.low\n",
    "    prev_close = prev.close\n",
    "    prev_open = prev.open\n",
    "    return {\n",
    "        \"day_high_gt_prev_high\": d.high > prev_high,\n",
    "        \"day_low_gt_prev_low\": d.low > prev_low,\n",
    "        \"day_open_gt_prev_open\": d.open > prev_open,\n",
    "        \"day_close_gt_prev_close\": d.close > prev_close,\n",
    "        \"day_high_lt_prev_high\": d.high < prev_high,\n",
    "        \"day_low_lt_prev_low\": d.low < prev_low,\n",
    "        \"day_open_lt_prev_open\": d.open < prev_open,\n",
    "        \"day_close_lt_prev_close\": d.close < prev_close,\n",
    "        \"day_open_eq_high\": d.open == d.high,\n",
    "        \"day_open_eq_low\": d.open == d.low,\n",
    "    }\n",
    "\n",
    "\n",
    "def pocket_pivot(d: pd.DataFrame, prev: pd.DataFrame):\n",
    "    price_check = d.close > prev.close\n",
    "    negative_volume = d.volume.where(d.close < prev.close, 0)\n",
    "    max_negative_vol_in10_days = negative_volume.rolling(window=10, min_periods=1).max()\n",
    "    volume_check = d.volume > max_negative_vol_in10_days\n",
    "    return price_check & volume_check\n",
    "\n",
    "\n",
    "def minicoil(d: pd.DataFrame, prev: pd.DataFrame, prev_2: pd.DataFrame):\n",
    "    return (\n",
    "            (prev.close < prev_2.close) & (prev.low > prev_2.low)  # day2_inside\n",
    "            &\n",
    "            (d.high < prev_2.high) & (d.low > prev_2.low)  # day1_inside\n",
    "    )\n",
    "\n",
    "\n",
    "def three_week_tight(w: pd.DataFrame):\n",
    "    rolling_3 = w.close.rolling(window=3)\n",
    "    # Range is within 1.5%\n",
    "    return ((rolling_3.max() - rolling_3.min()) / rolling_3.mean()) <= 0.015\n",
    "\n",
    "\n",
    "def five_week_up(w: pd.DataFrame):\n",
    "    w_close = w.close\n",
    "    w_close1 = w_close.shift(1)\n",
    "    w_close2 = w_close.shift(2)\n",
    "    w_close3 = w_close.shift(3)\n",
    "    w_close4 = w.close.shift(4)\n",
    "    w_close5 = w.close.shift(5)\n",
    "\n",
    "    return ((w_close > w_close1) &\n",
    "            (w_close1 > w_close2) &\n",
    "            (w_close2 > w_close3) &\n",
    "            (w_close3 > w_close4) &\n",
    "            (w_close4 > w_close5))\n",
    "\n",
    "\n",
    "def high_tight_flag(d: pd.DataFrame):\n",
    "    rolling8 = d.close.rolling(window=8)\n",
    "    rolling3_high = d.high.rolling(window=3)\n",
    "    rolling3_low = d.low.rolling(window=3)\n",
    "    rolling_3_close = d.close.rolling(window=3)\n",
    "    # 90% sharp move\n",
    "    sharp_move = rolling8.max() / rolling8.min() >= 1.90\n",
    "    # Tight (<= 25%) consolidation\n",
    "    consolidation = (rolling3_high.max() - rolling3_low.min()) / rolling_3_close.mean() <= 0.025\n",
    "    return sharp_move & consolidation\n",
    "\n",
    "\n",
    "def ants(d: pd.DataFrame, prev: pd.DataFrame):\n",
    "    return (\n",
    "            ((d.close > prev.close).rolling(window=15).sum() >= 12)  # 12/15 days up\n",
    "            &\n",
    "            (d.volume > d.volume.rolling(window=15).mean())  # Increase in average volume\n",
    "    )\n",
    "\n",
    "\n",
    "def power_trend(d: pd.DataFrame, ema_21: pd.Series, sma_50: pd.Series):\n",
    "    return (\n",
    "            (d.close > ema_21)  # Close above 21\n",
    "            &\n",
    "            (ema_21 > sma_50)  # 21 EMA > 50 SMA\n",
    "            &\n",
    "            (d.close > sma_50)  # Close above 50 SMA\n",
    "    )\n",
    "\n",
    "\n",
    "def power_of_three(d: pd.DataFrame, ema_10: pd.Series, ema_21: pd.Series, sma_50: pd.Series):\n",
    "    return (\n",
    "            ta.cross(d.close, ema_10)  # Close above 10\n",
    "            &\n",
    "            ta.cross(d.close, ema_21)  # Close above 21\n",
    "            &\n",
    "            ta.cross(d.close, sma_50)  # Close above 50 SMA\n",
    "    )\n",
    "\n",
    "\n",
    "def launchpad_daily(ema_21: pd.Series, sma_50: pd.Series):\n",
    "    # Short and long-term MAs close to each other (< 2%)\n",
    "    return (ema_21 / sma_50 - 1).abs() < 0.02\n",
    "\n",
    "\n",
    "def launchpad_weekly(ema_5: pd.Series, sma_10: pd.Series):\n",
    "    # Short and long-term MAs close to each other (< 2%)\n",
    "    return (ema_5 / sma_10 - 1).abs() < 0.02\n",
    "\n",
    "\n",
    "def sigma_spike(d: pd.DataFrame):\n",
    "    # Calculate daily percent change\n",
    "    day_chang_pct = ta.percent_return(d.close) * 100\n",
    "    # Calculate standard deviation of daily percent changes over the past 20 days\n",
    "    volatility_20_days = day_chang_pct.rolling(window=20).std()\n",
    "    # Calculate Sigma Spike\n",
    "    return day_chang_pct / volatility_20_days\n",
    "\n",
    "\n",
    "def sma_comparison(d: pd.DataFrame, sma_200: pd.Series):\n",
    "    return {\n",
    "        f\"sma_200_vs_sma_200_{i}M_ago\": sma_200 > ta.sma(d.close.shift(21 * i)) for i in range(7)\n",
    "    }\n",
    "\n",
    "\n",
    "def stan_weinstein_stage_analysis(d: pd.DataFrame):\n",
    "    # TODO\n",
    "    # Stan\n",
    "    # Weinstein\n",
    "    # Stages(1\n",
    "    # A, 1, 2\n",
    "    # A, 2, 3\n",
    "    # A, 3, 4, 4\n",
    "    # B -)  #\n",
    "    return \"-\"\n",
    "\n",
    "\n",
    "def sma_vs_ema_slope(d: pd.DataFrame, freq: str, period: list[int]):\n",
    "    cols = {}\n",
    "    for i in period:\n",
    "        ma = d.close.rolling(i).mean()\n",
    "        ma_shifted = ma.shift(1)\n",
    "        ma_slope = (ma - ma_shifted) / ma_shifted * 100\n",
    "        adr_10 = (d.high - d.low).rolling(10).mean() / d.close * 100\n",
    "        ma_vs_adr_10 = ma_slope / adr_10\n",
    "\n",
    "        cols[f\"sma_{i}{freq}\"] = ma\n",
    "        cols[f\"{i}{freq}sma_vs_ema_slope_pct\"] = ma_slope\n",
    "        cols[f\"{i}{freq}sma_vs_ema_slope_adr\"] = ma_vs_adr_10\n",
    "    return cols\n",
    "\n",
    "\n",
    "def relative_strength(d: pd.DataFrame, market_close: pd.Series):\n",
    "    rs_day_periods = [5, 10, 15, 20, 25, 30, 60, 90]\n",
    "\n",
    "    symbol_return = ta.percent_return(d.close)\n",
    "    market_return = ta.percent_return(market_close)\n",
    "    rs_day = symbol_return > market_return\n",
    "\n",
    "    # Relative Strength\n",
    "    cols = {}\n",
    "    for i in rs_day_periods:\n",
    "        days = rs_day.rolling(i).sum()\n",
    "        cols[f\"RS_{i}D\"] = days\n",
    "        cols[f\"RS_{i}D_pct\"] = days / i * 100\n",
    "\n",
    "    rsnh_period_month = [1, 3, 6, 9, 12]\n",
    "    rs_line = symbol_return / market_return\n",
    "    for i in rsnh_period_month:\n",
    "        # Relative Strength New High\n",
    "        widow = i * 21\n",
    "        rsnh = rs_line == rs_line.rolling(widow).max()\n",
    "        cols[f\"RSNH_{i}M\"] = rsnh\n",
    "\n",
    "        # Relative Strength New High Before Price\n",
    "        stock_high = d.close == d.close.rolling(i).max()\n",
    "        rsnhbp = rsnh & ~stock_high\n",
    "        cols[f\"RSNHBP_{i}M\"] = rsnhbp\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def adr(d: pd.DataFrame):\n",
    "    period = [1, 2, 5, 10, 14, 20]\n",
    "    cols = {}\n",
    "    for i in period:\n",
    "        rng = d.high - d.low\n",
    "        adr_value = rng.rolling(i).mean()\n",
    "        adr_pct = adr_value / d.close * 100\n",
    "        cols[f\"ADR_{i}D\"] = adr_value\n",
    "        cols[f\"ADR_pct_{i}D\"] = adr_pct\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def atr(d: pd.DataFrame):\n",
    "    period = [2, 5, 10, 14, 20]\n",
    "    cols = {}\n",
    "    for i in period:\n",
    "        cols[f\"ATR_{i}D\"] = ta.atr(d.high, d.low, d.close, i)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def alpha(d: pd.DataFrame, market_close: pd.Series):\n",
    "    #6 Month\n",
    "    market_return = ta.percent_return(market_close, 6 * 21)\n",
    "    symbol_return = ta.percent_return(d.close, 6 * 21)\n",
    "    cols = {\n",
    "        \"alpha_6M\": np.nan if market_return is None or symbol_return is None else (symbol_return - market_return) * 100,\n",
    "    }\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def safe_call_cdl_pattern(d: pd.DataFrame, name: str, bearish=False) -> pd.Series | bool:\n",
    "    # noinspection PyBroadException\n",
    "    try:\n",
    "        result = ta.cdl(d.open, d.high, d.low, d.close, name=name)\n",
    "        if len(result.columns) == 0:\n",
    "            result = None\n",
    "    except:\n",
    "        result = None\n",
    "\n",
    "    if result is None:\n",
    "        return False\n",
    "\n",
    "    series = result[result.columns[0]]\n",
    "    if bearish:\n",
    "        return series < 0\n",
    "    return series > 0\n",
    "\n",
    "\n",
    "def indicators(d: pd.DataFrame, w: pd.DataFrame, market_close: pd.Series):\n",
    "    # Link: https://deepvue.com/knowledge-base/technical/\n",
    "    prev = d.shift(1)\n",
    "    prev_2 = d.shift(2)\n",
    "    prev_w = w.shift(1)\n",
    "    ema_4_high = d.high.ewm(span=4).mean()\n",
    "    ema_10_close = d.close.ewm(span=10).mean()\n",
    "    ema_21_close = d.close.ewm(span=21).mean()\n",
    "    sma_50_close = d.close.rolling(50).mean()\n",
    "    w_ema_5_close = w.close.ewm(span=5).mean()\n",
    "    w_sma_10_close = w.close.rolling(10).mean()\n",
    "\n",
    "    inside = safe_call_cdl_pattern(d, name='inside')\n",
    "    inside_week = safe_call_cdl_pattern(w, name='inside')\n",
    "    cols = {\n",
    "        \"inside_day\": inside,\n",
    "        \"double_inside_day\": inside & safe_call_cdl_pattern(prev, name='inside'),\n",
    "        \"inside_week\": inside_week,\n",
    "        \"double_inside_week\": inside_week & safe_call_cdl_pattern(prev_w, name='inside'),\n",
    "        \"outside_day\": safe_call_cdl_pattern(d, name='engulfing'),\n",
    "        \"outside_week\": safe_call_cdl_pattern(w, name='engulfing'),\n",
    "        \"outside_bullish_day\": (d.open < prev.low) & (d.close > prev.high),\n",
    "        \"outside_bearish_day\": (d.open > prev.high) & (d.close < prev.low),\n",
    "        \"outside_bullish_week\": (w.open < prev_w.low) & (w.close > prev_w.high),\n",
    "        \"outside_bearish_week\": (w.open > prev_w.high) & (w.close < prev_w.low),\n",
    "        \"wick_play\": ((d.low > prev.open) | (d.low > prev.close)) & (d.high < prev.high),\n",
    "        \"in_the_wick\": (d.open < prev.high) & ((d.low > prev.low) | (d.open > prev.high)),\n",
    "        \"3_line_strike_bullish\": safe_call_cdl_pattern(d, name='3linestrike'),\n",
    "        \"3_line_strike_bearish\": safe_call_cdl_pattern(d, name='3linestrike', bearish=True),\n",
    "        \"3_bar_break\": d.close > prev.high.rolling(3).max(),\n",
    "        \"bullish_reversal\": (d.low < prev.low) & d.close > prev.close,\n",
    "        \"upside_reversal\": (d.low < prev.low) & (d.close > (d.high + d.low) / 2),\n",
    "        \"oops_reversal\": (d.open < prev.low) & (d.close > prev.low),\n",
    "        \"key_reversal\": (d.open < prev.low) & (d.close < prev.high),\n",
    "        \"pocket_pivot\": pocket_pivot(d, prev),\n",
    "        \"volume_dry_up\": d.volume == d.volume.rolling(window=10, min_periods=1).min(),\n",
    "        \"slingshot\": (d.close > ema_4_high) & (d.close <= ema_4_high.shift(1)),\n",
    "        \"minicoil\": minicoil(d, prev, prev_2),\n",
    "        \"3_week_tight\": three_week_tight(w),\n",
    "        \"5_week_up\": five_week_up(w),\n",
    "        \"high_tight_flag\": high_tight_flag(d),\n",
    "        \"ants\": ants(d, prev),\n",
    "        \"power_trend\": power_trend(d, ema_21_close, sma_50_close),\n",
    "        \"power_of_three\": power_of_three(d, ema_10_close, ema_21_close, sma_50_close),\n",
    "        \"launchpad\": launchpad_daily(ema_21_close, sma_50_close),\n",
    "        \"launchpad_weekly\": launchpad_weekly(w_ema_5_close, w_sma_10_close),\n",
    "        #TODO: Green Line Breakout\n",
    "        \"doji\": safe_call_cdl_pattern(d, name='doji'),\n",
    "        \"morning_star\": safe_call_cdl_pattern(d, name='morningstar'),\n",
    "        \"evening_star\": safe_call_cdl_pattern(d, name='eveningstar'),\n",
    "        \"shooting_star\": safe_call_cdl_pattern(d, name='shootingstar'),\n",
    "        \"hammer\": safe_call_cdl_pattern(d, name='hammer'),\n",
    "        \"inverted_hammer\": safe_call_cdl_pattern(d, name='invertedhammer'),\n",
    "        \"bullish_harami\": safe_call_cdl_pattern(d, name='harami'),\n",
    "        \"bearish_harami\": safe_call_cdl_pattern(d, name='harami', bearish=False),\n",
    "        #TODO: Bullish engulfing and bearish engulfing\n",
    "        #TODO: Bullish kicker and bearish engulfing,\n",
    "        \"piercing_line\": safe_call_cdl_pattern(d, name='piercing'),\n",
    "        \"hanging_man\": safe_call_cdl_pattern(d, name='hangingman'),\n",
    "        \"dark_cloud_cover\": safe_call_cdl_pattern(d, name='darkcloudcover'),\n",
    "        \"gravestone_doji\": safe_call_cdl_pattern(d, name='gravestonedoji'),\n",
    "        \"3_back_crows\": safe_call_cdl_pattern(d, name='3blackcrows'),\n",
    "        \"dragonfly_doji\": safe_call_cdl_pattern(d, name='dragonflydoji'),\n",
    "        \"3_white_soldiers\": safe_call_cdl_pattern(d, name='3whitesoldiers'),\n",
    "        \"sigma_spike\": sigma_spike(d),\n",
    "        \"stan_weinstein_stage\": stan_weinstein_stage_analysis(d),\n",
    "    }\n",
    "    return cols\n",
    "\n",
    "\n",
    "def technical(d: pd.DataFrame, w: pd.DataFrame, market_close: pd.Series, row):\n",
    "    cols = price_compare(d)\n",
    "    sma_200_close = d.close.rolling(200).mean()\n",
    "    try:\n",
    "        cols = cols | indicators(d, w, market_close)\n",
    "        #Alpha\n",
    "        cols = cols | alpha(d, market_close)\n",
    "        # SMA Comparison Months Back\n",
    "        cols = cols | sma_comparison(d, sma_200_close)\n",
    "        # SMA Comparison with EMA Slop\n",
    "        cols = cols | sma_vs_ema_slope(d, \"D\", [10, 20, 30, 40, 50, 100, 200])\n",
    "        # Relative Strength\n",
    "        cols = cols | relative_strength(d, market_close)\n",
    "        # ADR\n",
    "        cols = cols | adr(d)\n",
    "        # ATR\n",
    "        cols = cols | atr(d)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in technical indicators: \", row.name)\n",
    "        print(d.head())\n",
    "        print(market_close.head())\n",
    "        raise e\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_MahjDEfp6-"
   },
   "outputs": [],
   "source": [
    "def process_candles(row):\n",
    "    ticker = row.name\n",
    "    d = daily_candles[ticker]\n",
    "    w = weekly_candles[ticker]\n",
    "    m = monthly_candles[ticker]\n",
    "    y = yearly_candle[ticker]\n",
    "    market_close = daily_candles[market_ticker[row.exchange]].reindex(d.index).close.fillna(0)\n",
    "    if d.volume.empty:\n",
    "        d['volume'] = np.nan\n",
    "\n",
    "    last_trading_day = d.index[-1]\n",
    "    year_back = last_trading_day - pd.DateOffset(years=1)\n",
    "    d_w52 = d[d.index > year_back]\n",
    "\n",
    "    last_earning_date = row.earnings_release_trading_date\n",
    "    d_since_earning = d[d.index >= last_earning_date]\n",
    "\n",
    "    #Meta\n",
    "    days_since_earning = len(d_since_earning)\n",
    "    cols = {\"days_since_latest_earning\": pd.Series([days_since_earning])}\n",
    "\n",
    "    #OHLCV\n",
    "    cols = cols | ohlcv(d, name='day') | ohlcv(d, name='prev_day', shift=1)\n",
    "    cols = cols | ohlcv(w, name='week') | ohlcv(w, name='prev_week', shift=1)\n",
    "    cols = cols | ohlcv(m, name='month') | ohlcv(m, name='prev_month', shift=1)\n",
    "    cols = cols | ohlcv(y, name='year') | ohlcv(y, name='prev_year', shift=1)\n",
    "\n",
    "    daily_periods = [5, 10, 20, 30, 40, 50, 100, 200]\n",
    "    weekly_periods = [10, 20, 30, 40, 50]\n",
    "\n",
    "    # Price Action\n",
    "    cols = cols | price_action(d, d_w52, w, m, y, days_since_earning, last_trading_day, daily_periods, weekly_periods)\n",
    "\n",
    "    # Volume Action\n",
    "    cols = cols | volume_action(d, d_w52, d_since_earning, w, row, daily_periods, weekly_periods, last_trading_day)\n",
    "\n",
    "    # Technical\n",
    "    cols = cols | technical(d, w, market_close, row)\n",
    "\n",
    "    return {k: get_latest(v) for k, v in cols.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e64XdUQXfr4Z"
   },
   "outputs": [],
   "source": [
    "######################## ~~~Technical~~~~ ########################\n",
    "df['no_volume'] = q['has-no-volume']\n",
    "# Beta\n",
    "df['beta_1_year'] = q['beta_1_year']\n",
    "df['beta_3_year'] = q['beta_3_year']\n",
    "df['beta_5_year'] = q['beta_5_year']\n",
    "\n",
    "# To Weekly, Monthly and Yearly Candle\n",
    "market_ticker = {\"NSE\": \"NSE:CNX500\", \"BSE\": \"NSE:CNX500\"}\n",
    "weekly_candles = {ticker: to_weekly_candles(d) for ticker, d in daily_candles.items()}\n",
    "monthly_candles = {ticker: to_monthly_candles(d) for ticker, d in daily_candles.items()}\n",
    "yearly_candle = {ticker: to_yearly_candles(d) for ticker, d in monthly_candles.items()}\n",
    "ta_metrics_data = df.apply(process_candles, axis=1)\n",
    "ta_df: pd.DataFrame = ta_metrics_data.apply(pd.Series)\n",
    "df = df.join(ta_df)\n",
    "######################## ~~~Technical~~~~ ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S3gG0YHft17"
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqrSIHEHtSlj"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def detect_and_transform_json_columns(df: pd.DataFrame):\n",
    "    json_columns = {}\n",
    "    df: pd.DataFrame = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Only process object-type columns\n",
    "            # Check if any value in the column is a dict or list\n",
    "            if any(isinstance(val, (dict, list)) for val in df[col]):\n",
    "                # Mark the column for JSONB storage\n",
    "                json_columns[col] = JSONB\n",
    "        # Replace NaN with None for PostgreSQL compatibility\n",
    "        df[col] = df[col].where(pd.notnull(df[col]), None)\n",
    "\n",
    "    df = df.replace('nan', None)\n",
    "    return df, json_columns"
   ],
   "metadata": {
    "id": "WjtIHryPRWm4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "4ABzqoyiRXrf"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "export_df, json_columns = detect_and_transform_json_columns(df)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reset the temp table\n",
    "final_table = \"symbols\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    export_df.to_sql(final_table, conn, if_exists='replace', index=True, dtype=json_columns)\n",
    "    conn.execute(text(f\"ALTER TABLE {final_table} ADD PRIMARY KEY (ticker)\"))\n",
    "    conn.commit()\n",
    "    print(\"Table reset to symbols\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
